{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bring in relevant libraries\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import httplib, urllib, json, locale\n",
    "from urlparse import urlparse\n",
    "import operator\n",
    "import bson\n",
    "from collections import OrderedDict\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-185381317f5f>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-185381317f5f>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    db=connection.\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "merchant_ids = []\n",
    "#connecting to our db\n",
    "connection = MongoClient(host = '')\n",
    "db=connection.#removed this for privacy\n",
    "merchant = db['Merchants']\n",
    "for merchants in merchant.find():\n",
    "     merchant_ids.append(str(merchants['_id']))\n",
    "        \n",
    "#{'_id':ObjectId(MERCHANT_ID)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merchant_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-935c59325609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mMERCHANT_ID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerchant_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#eventually we'll want this to be brought in externally from mongo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#object_id = 'ObjectId(\\\"' + MERCHANT_ID + '\\\")'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merchant_ids' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for MERCHANT_ID in merchant_ids:\n",
    "    #eventually we'll want this to be brought in externally from mongo\n",
    "\n",
    "    #object_id = 'ObjectId(\\\"' + MERCHANT_ID + '\\\")'\n",
    "    doc1 = []\n",
    "\n",
    "    for doc in merchant.find({'_id':ObjectId(MERCHANT_ID)}):\n",
    "         doc1.append(doc)\n",
    "    try:\n",
    "        access_token = doc1[0]['pos_access_token']\n",
    "\n",
    "        record=db['Reports']\n",
    "\n",
    "        access_token = str(access_token)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #cron = CronTab()\n",
    "    #job = cron.new(command='/usr/bin/echo')\n",
    "    #job.day.every(1)\n",
    "\n",
    "\n",
    "    #!/usr/bin/python\n",
    "    #\n",
    "    # Generating a payment report with the Square Connect API.\n",
    "\n",
    "\n",
    "\n",
    "    # Standard HTTP headers for every Connect API request\n",
    "    request_headers = {'Authorization': 'Bearer ' + access_token,\n",
    "                       'Accept': 'application/json',\n",
    "                       'Content-Type': 'application/json'}\n",
    "\n",
    "    # The base URL for every Connect API request\n",
    "    connection = httplib.HTTPSConnection('connect.squareup.com')\n",
    "\n",
    "    # Uses the locale to format currency amounts correctly\n",
    "    locale.setlocale(locale.LC_ALL, 'en_US')\n",
    "\n",
    "    loc_ids = []\n",
    "\n",
    "    # Helper function to convert cent-based money amounts to dollars and cents\n",
    "    def format_money(amount):\n",
    "      return locale.currency(amount / 100.)\n",
    "\n",
    "\n",
    "    # Obtains all of the business's location IDs. Each location has its own collection of payments.\n",
    "    def get_location_ids():\n",
    "      request_path = '/v1/me/locations'\n",
    "      connection.request('GET', request_path, '', request_headers)\n",
    "      response = connection.getresponse()\n",
    "\n",
    "      # Transform the JSON array of locations into a Python list\n",
    "      locations = json.loads(response.read())\n",
    "\n",
    "      location_ids = []\n",
    "      for location in locations:\n",
    "        location_ids.append(location['id'])\n",
    "        loc_ids.append(location['id'])\n",
    "\n",
    "      return location_ids\n",
    "\n",
    "\n",
    "    # Downloads all of a business's payments\n",
    "    def get_payments_day(location_ids):\n",
    "\n",
    "    #get time right now, timezone should be correct if computer is synced properly\n",
    "      d = datetime.utcnow()\n",
    "      diff_time = datetime.utcnow() - timedelta(days = 1)\n",
    "\n",
    "\n",
    "      #time_now\n",
    "      # Make sure to URL-encode all parameters\n",
    "\n",
    "    #turn current time into a bunch of strings that we can add together to form link parameters below\n",
    "      year,month,day,hour,minute,second = str(d.year),str(d.month),str(d.day),str(d.hour),str(d.minute),str(d.second)\n",
    "      year_diff,month_diff,day_diff,hour_diff,minute_diff,second_diff = str(diff_time.year),str(diff_time.month),str(diff_time.day),str(diff_time.hour),str(diff_time.minute),str(diff_time.second)\n",
    "\n",
    "    #if any of our dates/times are only one digit, we add a '0' to the front to make them suitable for URL \n",
    "      if len(day) == 1:\n",
    "            day = '0'+ day\n",
    "      if len(month) == 1:\n",
    "            month = '0'+ month\n",
    "      if len(hour) == 1:\n",
    "            hour = '0' + hour\n",
    "      if len(minute) == 1:\n",
    "            minute = '0' + minute\n",
    "      if len(second) == 1:\n",
    "            second = '0' + second\n",
    "\n",
    "      if len(day_diff) == 1:\n",
    "            day_diff = '0'+ day_diff\n",
    "      if len(month_diff) == 1:\n",
    "            month_diff = '0'+ month_diff\n",
    "     #####there is a problem here with the days, I think it's when the next day is being called because UTC\n",
    "      parameters = urllib.urlencode({'begin_time': year_diff + '-' + month_diff + '-' + day_diff + 'T05:00:00',\n",
    "                                     'end_time': year + '-' + month + '-' + day + 'T05:00:00'})\n",
    "      payments = []\n",
    "\n",
    "      # For each location...\n",
    "      for location_id in location_ids:\n",
    "\n",
    "        print 'Downloading payments for location with ID ' + location_id + '...'\n",
    "\n",
    "        request_path = '/v1/' + location_id + '/payments?' + parameters\n",
    "        more_results = True\n",
    "\n",
    "        # ...as long as there are more payments to download from the location...\n",
    "        while more_results:\n",
    "\n",
    "          # ...send a GET request to /v1/LOCATION_ID/payments\n",
    "          connection.request('GET', request_path, '', request_headers)\n",
    "          response = connection.getresponse()\n",
    "\n",
    "          # Read the response body JSON into the cumulative list of results\n",
    "          payments = payments + json.loads(response.read())\n",
    "\n",
    "          # Check whether pagination information is included in a response header, indicating more results\n",
    "          pagination_header = response.getheader('link', '')\n",
    "          if \"rel='next'\" not in pagination_header:\n",
    "            more_results = False\n",
    "          else:\n",
    "\n",
    "            # Extract the next batch URL from the header.\n",
    "            #\n",
    "            # Pagination headers have the following format:\n",
    "            # <https://connect.squareup.com/v1/LOCATION_ID/payments?batch_token=BATCH_TOKEN>;rel='next'\n",
    "            # This line extracts the URL from the angle brackets surrounding it.\n",
    "            next_batch_url = urlparse(pagination_header.split('<')[1].split('>')[0])\n",
    "\n",
    "            request_path = next_batch_url.path + '?' + next_batch_url.query\n",
    "\n",
    "      # Remove potential duplicate values from the list of payments\n",
    "      seen_payment_ids = set()\n",
    "      unique_payments = []\n",
    "\n",
    "      for payment in payments:\n",
    "        if payment['id'] in seen_payment_ids:\n",
    "          continue\n",
    "        seen_payment_ids.add(payment['id'])\n",
    "        unique_payments.append(payment)\n",
    "\n",
    "      return unique_payments\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "      # Get all payments from all of the business's locations\n",
    "      payments = get_payments_day(get_location_ids())\n",
    "      pay_data = payments\n",
    "      # Print a sales summary report of the payments\n",
    "      #print_sales_report(payments)\n",
    "\n",
    "\n",
    "\n",
    "    loc_ids = [str(id) for id in loc_ids]\n",
    "\n",
    "\n",
    "\n",
    "    #!/usr/bin/python\n",
    "    #\n",
    "    # Generating a payment report with the Square Connect API.\n",
    "\n",
    "\n",
    "\n",
    "    # Get this from your application dashboard (https://connect.squareup.com/apps)\n",
    "    # this is secret, don't share\n",
    "    \n",
    "    \n",
    "    #access_token = 'sq0atp-syWTe9WU-JJlDucTqFgZfw'\n",
    "\n",
    "    # Standard HTTP headers for every Connect API request\n",
    "    request_headers = {'Authorization': 'Bearer ' + access_token,\n",
    "                       'Accept': 'application/json',\n",
    "                       'Content-Type': 'application/json'}\n",
    "\n",
    "    # The base URL for every Connect API request\n",
    "    connection = httplib.HTTPSConnection('connect.squareup.com')\n",
    "\n",
    "    # Uses the locale to format currency amounts correctly\n",
    "    locale.setlocale(locale.LC_ALL, 'en_US')\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    # Downloads all of a business's payments\n",
    "    def get_payments_month(location_ids):\n",
    "\n",
    "    #get time right now, timezone should be correct if computer is synced properly\n",
    "      d = datetime.utcnow()\n",
    "      diff_time = datetime.utcnow() - timedelta(weeks = 4)\n",
    "\n",
    "\n",
    "      #time_now\n",
    "      # Make sure to URL-encode all parameters\n",
    "\n",
    "    #turn current time into a bunch of strings that we can add together to form link parameters below\n",
    "      year,month,day,hour,minute,second = str(d.year),str(d.month),str(d.day),str(d.hour),str(d.minute),str(d.second)\n",
    "      year_diff,month_diff,day_diff,hour_diff,minute_diff,second_diff = str(diff_time.year),str(diff_time.month),str(diff_time.day),str(diff_time.hour),str(diff_time.minute),str(diff_time.second)\n",
    "\n",
    "    #if any of our dates/times are only one digit, we add a '0' to the front to make them suitable for URL \n",
    "      if len(day) == 1:\n",
    "            day = '0'+ day\n",
    "      if len(month) == 1:\n",
    "            month = '0'+ month\n",
    "      if len(hour) == 1:\n",
    "            hour = '0' + hour\n",
    "      if len(minute) == 1:\n",
    "            minute = '0' + minute\n",
    "      if len(second) == 1:\n",
    "            second = '0' + second\n",
    "\n",
    "      if len(day_diff) == 1:\n",
    "            day_diff = '0'+ day_diff\n",
    "      if len(month_diff) == 1:\n",
    "            month_diff = '0'+ month_diff\n",
    "\n",
    "      parameters = urllib.urlencode({'begin_time': year_diff + '-' + month_diff + '-' + day_diff + 'T05:00:00',\n",
    "                                     'end_time': year + '-' + month + '-' + day + 'T05:00:00'})\n",
    "      payments = []\n",
    "\n",
    "      # For each location...\n",
    "      for location_id in location_ids:\n",
    "\n",
    "        print 'Downloading payments for location with ID ' + location_id + '...'\n",
    "\n",
    "        request_path = '/v1/' + location_id + '/payments?' + parameters\n",
    "        more_results = True\n",
    "\n",
    "        # ...as long as there are more payments to download from the location...\n",
    "        while more_results:\n",
    "\n",
    "          # ...send a GET request to /v1/LOCATION_ID/payments\n",
    "          connection.request('GET', request_path, '', request_headers)\n",
    "          response = connection.getresponse()\n",
    "\n",
    "          # Read the response body JSON into the cumulative list of results\n",
    "          payments = payments + json.loads(response.read())\n",
    "\n",
    "          # Check whether pagination information is included in a response header, indicating more results\n",
    "          pagination_header = response.getheader('link', '')\n",
    "          if \"rel='next'\" not in pagination_header:\n",
    "            more_results = False\n",
    "          else:\n",
    "\n",
    "            # Extract the next batch URL from the header.\n",
    "            #\n",
    "            # Pagination headers have the following format:\n",
    "            # <https://connect.squareup.com/v1/LOCATION_ID/payments?batch_token=BATCH_TOKEN>;rel='next'\n",
    "            # This line extracts the URL from the angle brackets surrounding it.\n",
    "            next_batch_url = urlparse(pagination_header.split('<')[1].split('>')[0])\n",
    "\n",
    "            request_path = next_batch_url.path + '?' + next_batch_url.query\n",
    "\n",
    "      # Remove potential duplicate values from the list of payments\n",
    "      seen_payment_ids = set()\n",
    "      unique_payments = []\n",
    "\n",
    "      for payment in payments:\n",
    "        if payment['id'] in seen_payment_ids:\n",
    "          continue\n",
    "        seen_payment_ids.add(payment['id'])\n",
    "        unique_payments.append(payment)\n",
    "\n",
    "      return unique_payments\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "      # Get all payments from all of the business's locations\n",
    "      payments = get_payments_month(get_location_ids())\n",
    "      pay_data_month = payments\n",
    "      # Print a sales summary report of the payments\n",
    "      #print_sales_report(payments)\n",
    "\n",
    "\n",
    "    #!/usr/bin/python\n",
    "    #\n",
    "    # Demonstrates generating a payment report with the Square Connect API.\n",
    "    #\n",
    "    # This sample assumes all monetary amounts are in US dollars. You can alter the\n",
    "    # format_money function to display amounts in other currency formats.\n",
    "    #\n",
    "    # To run this script from the command line:\n",
    "    # python payments-report.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Standard HTTP headers for every Connect API request\n",
    "    request_headers = {'Authorization': 'Bearer ' + access_token,\n",
    "                       'Accept': 'application/json',\n",
    "                       'Content-Type': 'application/json'}\n",
    "\n",
    "    # The base URL for every Connect API request\n",
    "    connection = httplib.HTTPSConnection('connect.squareup.com')\n",
    "\n",
    "    # Uses the locale to format currency amounts correctly\n",
    "    locale.setlocale(locale.LC_ALL, 'en_US')\n",
    "\n",
    "\n",
    "\n",
    "    # Downloads all of a business's payments\n",
    "    def get_payments_week(location_ids):\n",
    "\n",
    "    #get time right now, timezone should be correct if computer is synced properly\n",
    "      d = datetime.utcnow()\n",
    "      diff_time = datetime.utcnow() - timedelta(weeks=1)\n",
    "\n",
    "    # Make sure to URL-encode all parameters\n",
    "\n",
    "    #turn current time into a bunch of strings that we can add together to form link parameters below\n",
    "      year,month,day,hour,minute,second = str(d.year),str(d.month),str(d.day),str(d.hour),str(d.minute),str(d.second)\n",
    "      year_diff,month_diff,day_diff,hour_diff,minute_diff,second_diff = str(diff_time.year),str(diff_time.month),str(diff_time.day),str(diff_time.hour),str(diff_time.minute),str(diff_time.second)\n",
    "\n",
    "    #if any of our dates/times are only one digit, we add a '0' to the front to make them suitable for URL \n",
    "      if len(day) == 1:\n",
    "            day = '0'+ day\n",
    "      if len(month) == 1:\n",
    "            month = '0'+ month\n",
    "      if len(hour) == 1:\n",
    "            hour = '0' + hour\n",
    "      if len(minute) == 1:\n",
    "            minute = '0' + minute\n",
    "      if len(second) == 1:\n",
    "            second = '0' + second\n",
    "\n",
    "      if len(day_diff) == 1:\n",
    "            day_diff = '0'+ day_diff\n",
    "      if len(month_diff) == 1:\n",
    "            month_diff = '0'+ month_diff\n",
    "\n",
    "      parameters = urllib.urlencode({'begin_time': year_diff + '-' + month_diff + '-' + day_diff + 'T05:00:00',\n",
    "                                     'end_time': year + '-' + month + '-' + day + 'T05:00:00'})\n",
    "      payments = []\n",
    "\n",
    "      # For each location...\n",
    "      for location_id in location_ids:\n",
    "\n",
    "        print 'Downloading payments for location with ID ' + location_id + '...'\n",
    "\n",
    "        request_path = '/v1/' + location_id + '/payments?' + parameters\n",
    "        more_results = True\n",
    "\n",
    "        # ...as long as there are more payments to download from the location...\n",
    "        while more_results:\n",
    "\n",
    "          # ...send a GET request to /v1/LOCATION_ID/payments\n",
    "          connection.request('GET', request_path, '', request_headers)\n",
    "          response = connection.getresponse()\n",
    "\n",
    "          # Read the response body JSON into the cumulative list of results\n",
    "          payments = payments + json.loads(response.read())\n",
    "\n",
    "          # Check whether pagination information is included in a response header, indicating more results\n",
    "          pagination_header = response.getheader('link', '')\n",
    "          if \"rel='next'\" not in pagination_header:\n",
    "            more_results = False\n",
    "          else:\n",
    "\n",
    "            # Extract the next batch URL from the header.\n",
    "            #\n",
    "            # Pagination headers have the following format:\n",
    "            # <https://connect.squareup.com/v1/LOCATION_ID/payments?batch_token=BATCH_TOKEN>;rel='next'\n",
    "            # This line extracts the URL from the angle brackets surrounding it.\n",
    "            next_batch_url = urlparse(pagination_header.split('<')[1].split('>')[0])\n",
    "\n",
    "            request_path = next_batch_url.path + '?' + next_batch_url.query\n",
    "\n",
    "      # Remove potential duplicate values from the list of payments\n",
    "      seen_payment_ids = set()\n",
    "      unique_payments = []\n",
    "\n",
    "      for payment in payments:\n",
    "        if payment['id'] in seen_payment_ids:\n",
    "          continue\n",
    "        seen_payment_ids.add(payment['id'])\n",
    "        unique_payments.append(payment)\n",
    "\n",
    "      return unique_payments\n",
    "\n",
    "\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "      # Get all payments from all of the business's locations\n",
    "      payments = get_payments_week(get_location_ids())\n",
    "      pay_data_week = payments\n",
    "      # Print a sales summary report of the payments\n",
    "      #print_sales_report(payments)\n",
    "\n",
    "      connection.close()\n",
    "\n",
    "    #access_token = doc1[0]['pos_access_token']    \n",
    "\n",
    "    print loc_ids[0]\n",
    "    print access_token\n",
    "    #list #list items, list payments\n",
    "\n",
    "\n",
    "    # In addition to an Authorization header, requests to the Connect API should\n",
    "    # include the indicated Accept and Content-Type headers.\n",
    "    request_headers = {'Authorization': 'Bearer ' + access_token,\n",
    "                   'Accept':        'application/json',\n",
    "                   'Content-Type':  'application/json'}\n",
    "\n",
    "    # Send a GET request to the ListLocations endpoint and obtain the response.\n",
    "    connection = httplib.HTTPSConnection('connect.squareup.com')\n",
    "    request_path = '/v1/'+loc_ids[0]+'/items'\n",
    "    connection.request('GET', request_path, '', request_headers)\n",
    "    response = connection.getresponse()\n",
    "\n",
    "\n",
    "\n",
    "    # Convert the returned JSON body into an array of locations you can work with.\n",
    "    items_data = json.loads(response.read())\n",
    "\n",
    "    # Pretty-print the locations array.\n",
    "    #locations = json.dumps(locations, indent=2, separators=(',',': '))\n",
    "\n",
    "    #making empty lists to fill with JSON pulled data\n",
    "    name_list = []\n",
    "    quant_list = []\n",
    "    quant_id_list = []\n",
    "    time_list = []\n",
    "    #pulling in item name,quantity purchased for the day, id of item, and time of purchase\n",
    "    for i in pay_data:\n",
    "        for n in range(10):\n",
    "            #all of our entries should be complete every time, if not, we'll add a nan\n",
    "            try:\n",
    "                name_list.append(i['itemizations'][n]['name'])\n",
    "            except:\n",
    "                name_list.append(np.nan)\n",
    "            try:\n",
    "                quant_list.append(str(i['itemizations'][n]['quantity']))\n",
    "            except:\n",
    "                quant_list.append(0)\n",
    "            try:\n",
    "                quant_id_list.append(i['itemizations'][n]['item_detail']['item_id'])\n",
    "            except:\n",
    "                quant_id_list.append(np.nan)\n",
    "            try:\n",
    "                time_list.append(i['created_at'])\n",
    "            except:\n",
    "                time_list.append(np.nan)\n",
    "\n",
    "\n",
    "\n",
    "    id_list = []\n",
    "    url_list = []\n",
    "    names_list = []\n",
    "    price_list = []\n",
    "    #pulling in item, id, price of item, and image url\n",
    "    for m in items_data:\n",
    "        #try and pull in data, if it doesn't exist for the instance, add a nan to be handled later\n",
    "        try:      \n",
    "            names_list.append(m['name'])\n",
    "        except:\n",
    "            names_list.append(np.nan)\n",
    "        try:\n",
    "            id_list.append(m['id'])\n",
    "        except:\n",
    "            id_list.append(np.nan)\n",
    "        try:\n",
    "            url_list.append(m['master_image']['url'])\n",
    "        except:\n",
    "            url_list.append(np.nan)\n",
    "        try:\n",
    "            price_list.append(m['variations'][0]['price_money']['amount'])\n",
    "        except:\n",
    "            price_list.append(np.nan)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #making empty lists to fill with JSON pulled data\n",
    "    name_list_mon = []\n",
    "    quant_list_mon = []\n",
    "    quant_id_list_mon = []\n",
    "    time_list_mon = []\n",
    "    #pulling in item name,quantity purchased for the day, id of item, and time of purchase\n",
    "    for i in pay_data_month:\n",
    "        for n in range(10):\n",
    "            #all of our entries should be complete every time, if not, we'll add a nan\n",
    "            try:\n",
    "                name_list_mon.append(i['itemizations'][n]['name'])\n",
    "            except:\n",
    "                name_list_mon.append(np.nan)\n",
    "            try:\n",
    "                quant_list_mon.append(str(i['itemizations'][n]['quantity']))\n",
    "            except:\n",
    "                quant_list_mon.append(0)\n",
    "            try:\n",
    "                quant_id_list_mon.append(i['itemizations'][n]['item_detail']['item_id'])\n",
    "            except:\n",
    "                quant_id_list_mon.append(np.nan)\n",
    "            try:\n",
    "                time_list_mon.append(i['created_at'])\n",
    "            except:\n",
    "                time_list_mon.append(np.nan)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #making empty lists to fill with JSON pulled data\n",
    "    name_list_week = []\n",
    "    quant_list_week = []\n",
    "    quant_id_list_week = []\n",
    "    time_list_week = []\n",
    "    #pulling in item name,quantity purchased for the day, id of item, and time of purchase\n",
    "    for i in pay_data_week:\n",
    "        for n in range(10):\n",
    "            #all of our entries should be complete every time, if not, we'll add a nan\n",
    "            try:\n",
    "                name_list_week.append(i['itemizations'][n]['name'])\n",
    "            except:\n",
    "                name_list_week.append(np.nan)\n",
    "            try:\n",
    "                quant_list_week.append(str(i['itemizations'][n]['quantity']))\n",
    "            except:\n",
    "                quant_list_week.append(0)\n",
    "            try:\n",
    "                quant_id_list_week.append(i['itemizations'][n]['item_detail']['item_id'])\n",
    "            except:\n",
    "                quant_id_list_week.append(np.nan)\n",
    "            try:\n",
    "                time_list_week.append(i['created_at'])\n",
    "            except:\n",
    "                time_list_week.append(np.nan)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    stop_words = ['drink','bag','gift','card','sauce','dip','soda','coke','pepsi','sprite','cola','beer','beverage','extra','crumbles']\n",
    "\n",
    "    #create data frame for each item on the menu and all the relevant information about it\n",
    "    df2 = pd.DataFrame({'id':id_list,'url':url_list, 'name':names_list, 'price':price_list})\n",
    "    df2.price = df2.price/100\n",
    "\n",
    "    #the items without a name are not typical menu items, so we get rid of those entries.\n",
    "    df2 = df2.dropna(subset = ['name']).reset_index()\n",
    "\n",
    "\n",
    "    #Compile the payment data into a dataframe\n",
    "    df_daily = pd.DataFrame({'quantity':quant_list,'id':quant_id_list,'item_name':name_list,'time_of_sale':time_list})\n",
    "\n",
    "\n",
    "    #mergethe dataframes on ID, but maintain\n",
    "    fin_df_daily = df_daily.merge(df2, on = 'id', how = 'outer')\n",
    "\n",
    "    #we aren't interest in all of the columns from the original two dataframe, so we take the ones we want\n",
    "    fin_df_daily = fin_df_daily[['name','quantity','id','price','url','time_of_sale']]\n",
    "    \n",
    "    print(fin_df_daily.head())\n",
    "    #we are turning quantity into a numeric so we can aggregate it to get totala quantity sold in our group by below\n",
    "    fin_df_daily.quantity = pd.to_numeric(fin_df_daily.loc[:,'quantity'])\n",
    "\n",
    "    #groupby quantity to get total sold for each product\n",
    "    fin_df_daily = fin_df_daily.groupby(['name','price']).sum().reset_index().sort_values('quantity', ascending = False)[['name','price','quantity']]\n",
    "\n",
    "    #fin_df_daily = fin_df_daily.dropna(subset = ['quantity']).reset_index(drop=True).head(40)\n",
    "\n",
    "\n",
    "\n",
    "    fin_df_daily.quantity[np.isnan(fin_df_daily.quantity)] = 0\n",
    "    df_item_subset = df2[['url','id','name']]\n",
    "    fin_df_daily = fin_df_daily.merge(df_item_subset, how = 'left', on='name')\n",
    "    fin_df_daily.name = fin_df_daily.name.str.lower()\n",
    "\n",
    "    indexes = [True]*len(fin_df_daily.name)\n",
    "\n",
    "    for i, name in enumerate(fin_df_daily.name):\n",
    "        for word in stop_words:\n",
    "            if word in name:\n",
    "                indexes[i]=False\n",
    "\n",
    "\n",
    "    fin_df_daily = fin_df_daily[indexes]\n",
    "\n",
    "    daily_hot = fin_df_daily[0:3]\n",
    "    daily_slow = fin_df_daily[-3:]\n",
    "\n",
    "\n",
    "    #get name and quantity to faciliate plotting\n",
    "    x_day = fin_df_daily[fin_df_daily.name!= 0].name\n",
    "    y = fin_df_daily[fin_df_daily.name!= 0].quantity\n",
    "    print y\n",
    "    y_day = y.astype(int)\n",
    "\n",
    "    #bar plots want a list of integers for their x-values\n",
    "    x_r = range(len(x_day))\n",
    "    x_g = x_day\n",
    "    if len(x_day)>40:\n",
    "        x_r = x_r[0:40]\n",
    "        y_g = y_day[0:40]\n",
    "        x_g = x_g[0:40]\n",
    "    \n",
    "\n",
    "    plt.bar(x_r,y_g, align='center')\n",
    "\n",
    "    #bring in the labels (names) and rotate the labels to make them easy to read\n",
    "    plt.xticks(x_r,x_g, rotation = 90)\n",
    "    plt.title('Daily top productss')\n",
    "    plt.ylabel('Quantity Purchased')\n",
    "    plt.show()\n",
    "  \n",
    "    fin_df_daily.head(20)\n",
    "\n",
    "\n",
    "    #Compile the payment data into a dataframe\n",
    "    df_weekly = pd.DataFrame({'quantity':quant_list_week,'id':quant_id_list_week,'item_name':name_list_week,'time_of_sale':time_list_week})\n",
    "\n",
    "\n",
    "    #mergethe dataframes on ID, but maintain\n",
    "    fin_df_week = df_weekly.merge(df2, on = 'id', how = 'outer')\n",
    "\n",
    "    #we aren't interest in all of the columns from the original two dataframe, so we take the ones we want\n",
    "    fin_df_week = fin_df_week[['name','quantity','id','price','url','time_of_sale']]\n",
    "\n",
    "    #we are turning quantity into a numeric so we can aggregate it to get totala quantity sold in our group by below\n",
    "    fin_df_week.quantity = pd.to_numeric(fin_df_week.loc[:,'quantity'])\n",
    "    fin_df_week = fin_df_week[fin_df_week.name.str.contains(\"drink\") == False]\n",
    "    fin_df_week = fin_df_week[fin_df_week.name.str.contains(\"Drink\") == False]\n",
    "\n",
    "    #groupby quantity to get total sold for each product\n",
    "    fin_df_week = fin_df_week.groupby(['name','price']).sum().reset_index().sort_values('quantity', ascending = False)[['name','price','quantity']]\n",
    "\n",
    "    fin_df_week.quantity[np.isnan(fin_df_week.quantity)] = 0\n",
    "    df_item_subset = df2[['url','id','name']]\n",
    "    fin_df_week = fin_df_week.merge(df_item_subset, how = 'left', on='name')\n",
    "    fin_df_week.name = fin_df_week.name.str.lower()\n",
    "\n",
    "    indexes = [True]*len(fin_df_week.name)\n",
    "\n",
    "    #pulling out any rows which have our stop words in the name column\n",
    "    for i, name in enumerate(fin_df_week.name):\n",
    "        for word in stop_words:\n",
    "            if word in name:\n",
    "                indexes[i]=False\n",
    "\n",
    "\n",
    "    fin_df_week = fin_df_week[indexes]\n",
    "\n",
    "\n",
    "    week_hot = fin_df_week[0:3]\n",
    "    week_slow = fin_df_week[-3:]\n",
    "\n",
    "\n",
    "    #get name and quantity to faciliate plotting\n",
    "    x_week = fin_df_week[fin_df_week.quantity!= 0].name\n",
    "    y = fin_df_week[fin_df_week.quantity!= 0].quantity\n",
    "\n",
    "    y_week = y.astype(int)\n",
    "\n",
    "    #bar plots want a list of integers for their x-values\n",
    "    x_r = range(len(x_week))\n",
    "    x_g = x_week\n",
    "    if len(x_day)>40:\n",
    "        x_r = x_r[0:40]\n",
    "        y_g = y_week[0:40]\n",
    "        x_g = x_g[0:40]\n",
    "        \n",
    "    \n",
    "    plt.bar(x_r,y_g, align='center')\n",
    "\n",
    "    #bring in the labels (names) and rotate the labels to make them easy to read\n",
    "    plt.xticks(x_r,x_g, rotation = 90)\n",
    "    plt.title('Weekly Product Movement')\n",
    "    plt.ylabel('Quantity Purchased')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #Compile the payment data into a dataframe\n",
    "    df_month = pd.DataFrame({'quantity':quant_list_mon,'id':quant_id_list_mon,'item_name':name_list_mon,'time_of_sale':time_list_mon})\n",
    "\n",
    "\n",
    "    #mergethe dataframes on ID, but maintain\n",
    "    fin_df_mon = df_month.merge(df2, on = 'id', how = 'outer')\n",
    "\n",
    "    #we aren't interest in all of the columns from the original two dataframe, so we take the ones we want\n",
    "    fin_df_mon = fin_df_mon[['name','quantity','id','price','url','time_of_sale']]\n",
    "\n",
    "    #we are turning quantity into a numeric so we can aggregate it to get totala quantity sold in our group by below\n",
    "    fin_df_mon.quantity = pd.to_numeric(fin_df_mon.loc[:,'quantity'])\n",
    "    fin_df_mon = fin_df_mon[fin_df_mon.name.str.contains(\"drink\") == False]\n",
    "    fin_df_mon = fin_df_mon[fin_df_mon.name.str.contains(\"Drink\") == False]\n",
    "\n",
    "    #groupby quantity to get total sold for each product\n",
    "    fin_df_mon = fin_df_mon.groupby(['name','price']).sum().reset_index().sort_values('quantity', ascending = False)[['name','price','quantity']]\n",
    "\n",
    "    fin_df_mon.quantity[np.isnan(fin_df_mon.quantity)] = 0\n",
    "    df_item_subset = df2[['url','id','name']]\n",
    "    fin_df_mon = fin_df_mon.merge(df_item_subset, how = 'left', on='name')\n",
    "    fin_df_mon.name = fin_df_mon.name.str.lower()\n",
    "\n",
    "    indexes = [True]*len(fin_df_mon.name)\n",
    "\n",
    "    for i, name in enumerate(fin_df_mon.name):\n",
    "        for word in stop_words:\n",
    "            if word in name:\n",
    "                indexes[i]=False\n",
    "                \n",
    "    fin_df_mon = fin_df_mon[indexes]\n",
    "\n",
    "    mon_hot = fin_df_mon[0:3]\n",
    "    mon_slow = fin_df_mon[-3:]\n",
    "\n",
    "\n",
    "    #get name and quantity to faciliate plotting\n",
    "    x = fin_df_mon[fin_df_mon.quantity!= 0].name\n",
    "    y = fin_df_mon[fin_df_mon.quantity!= 0].quantity\n",
    "\n",
    "    x_mon = x\n",
    "    x_g = x\n",
    "    y_mon = y.astype(int)\n",
    "\n",
    "    #bar plots want a list of integers for their x-values\n",
    "    x_r = range(len(x_mon))\n",
    "\n",
    "    \n",
    "    if len(x_day)>40:\n",
    "        x_r = x_r[0:40]\n",
    "        y_g = y_mon[0:40]\n",
    "        x_g = x_mon[0:40]\n",
    "        \n",
    "\n",
    "    plt.bar(x_r,y_g, align='center')\n",
    "\n",
    "    #bring in the labels (names) and rotate the labels to make them easy to read\n",
    "    plt.xticks(x_r,x_g, rotation = 90)\n",
    "    plt.title('Monthly Product Movement')\n",
    "    plt.ylabel('Quantity Purchased')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #print('Hot Products:\\n{}\\n{}\\n{}' .format(x_mon[0],x_mon[1],x_mon[2]))\n",
    "\n",
    "    #print('Slow Products:\\n{}' .format(x_mon[-3:]))\n",
    "\n",
    "    #convert from string to datetime\n",
    "    df_daily['time_of_sale'] = pd.to_datetime(df_daily['time_of_sale'])\n",
    "    #subtract 8 hours from our time of sale to get SF time of sale\n",
    "    df_daily['time_of_sale'] = df_daily.time_of_sale - timedelta(hours = 7)\n",
    "\n",
    "\n",
    "    #function to determine whether the sale was around breakfast, lunch, or dinner/evening\n",
    "    def t_o_d(time):\n",
    "        if time.hour < 11 and time.hour >= 5:\n",
    "            return 'Breakfast'\n",
    "        if time.hour >= 11 and time.hour < 17:\n",
    "             return 'Lunch'\n",
    "        if time.hour >= 17 or time.hour < 5:\n",
    "             return 'Dinner'\n",
    "\n",
    "    #function to determine whether the sale was on a weekend or weekday        \n",
    "    def week_weekend (time):\n",
    "        if time.weekday() != 'saturday' and time.weekday() != 'sunday':\n",
    "            return 'weekday'\n",
    "        else:\n",
    "            return 'weekend'\n",
    "\n",
    "    #convert from string to datetime\n",
    "    df_weekly['time_of_sale'] = pd.to_datetime(df_weekly['time_of_sale'])\n",
    "    #subtract 8 hours from our time of sale to get SF time of sale\n",
    "    df_weekly['time_of_sale'] = df_weekly.time_of_sale - timedelta(hours = 7)\n",
    "\n",
    "\n",
    "\n",
    "    #convert from string to datetime\n",
    "    df_month['time_of_sale'] = pd.to_datetime(df_month['time_of_sale'])\n",
    "    #subtract 8 hours from our time of sale to get SF time of sale\n",
    "    df_month['time_of_sale'] = df_month.time_of_sale - timedelta(hours = 7)\n",
    "\n",
    "\n",
    "\n",
    "    #if quantity is NaN then we know its a worthless entry, so we drop it\n",
    "    df_daily = df_daily.dropna(subset=['quantity']).reset_index(drop=True)\n",
    "\n",
    "    #apply our time of day function to our time of sale function\n",
    "    df_daily['time_of_day'] = df_daily.time_of_sale.apply(t_o_d)\n",
    "\n",
    "    #apply the weekday/weekend function\n",
    "    df_daily['week_weekend'] = df_daily.time_of_sale.apply(week_weekend)\n",
    "\n",
    "    #if quantity is NaN then we know its a worthless entry, so we drop it\n",
    "    df_weekly = df_weekly.dropna(subset=['quantity']).reset_index(drop=True)\n",
    "\n",
    "    #apply our time of day function to our time of sale function\n",
    "    df_weekly['time_of_day'] = df_weekly.time_of_sale.apply(t_o_d)\n",
    "\n",
    "    #apply the weekday/weekend function\n",
    "    df_weekly['week_weekend'] = df_weekly.time_of_sale.apply(week_weekend)\n",
    "\n",
    "    #if quantity is NaN then we know its a worthless entry, so we drop it\n",
    "    df_month = df_month.dropna(subset=['quantity']).reset_index(drop=True)\n",
    "\n",
    "    #apply our time of day function to our time of sale function\n",
    "    df_month['time_of_day'] = df_month.time_of_sale.apply(t_o_d)\n",
    "\n",
    "    #apply the weekday/weekend function\n",
    "    df_month['week_weekend'] = df_month.time_of_sale.apply(week_weekend)\n",
    "\n",
    "    ### Pymongo actually works in either BSON or Dict's, so this was worthless\n",
    "    json_week_hot = week_hot.to_json(orient = 'index')\n",
    "    json_week_slow = week_slow.to_json(orient = 'index')\n",
    "    json_day_hot = daily_hot.to_json(orient = 'index')\n",
    "    json_day_slow = daily_slow.to_json(orient = 'index')\n",
    "\n",
    "    print(str(json_week_hot))\n",
    "    print(str(json_week_slow))\n",
    "    print(str(json_day_hot))\n",
    "    print(str(json_day_slow))\n",
    "\n",
    "    json_list = [json_day_hot,json_day_slow,json_week_hot, json_week_slow]\n",
    "\n",
    "    #making dictionaries out of our df, 'records' maintains row data\n",
    "    dict_week_hot = week_hot.to_dict('records')\n",
    "    dict_week_slow = week_slow.to_dict('records')\n",
    "    dict_day_slow = daily_slow.to_dict('records')\n",
    "    dict_day_hot = daily_hot.to_dict('records')\n",
    "    dict_mon_hot = mon_hot.to_dict('records')\n",
    "    dict_mon_slow = mon_slow.to_dict('records')\n",
    "\n",
    "    dict_sold_day = OrderedDict(zip(x_day,y_day))\n",
    "    dict_sold_week = OrderedDict(zip(x_week,y_week))\n",
    "    dict_sold_mon = OrderedDict(zip(x_mon,y_mon))\n",
    "\n",
    "\n",
    "    #make a list of our dictionaries. We could iterate through this using UpdateMany(), but I prefer explicit indices they're \n",
    "    #easier to debug\n",
    "    #dict_list = [dict_day_hot,dict_day_slow,dict_week_hot, dict_week_slow]\n",
    "\n",
    "\n",
    "    #update_many allows all of our data to be brought in.\n",
    "    #Each of these dictionaries is actually a list of items in a dictionary\n",
    "    record.update_many({\n",
    "      'merchant_uid': MERCHANT_ID\n",
    "    },{\n",
    "      '$set': {\n",
    "        'top_3_items_of_day': dict_day_hot,\n",
    "        'top_3_items_of_week':dict_week_hot,\n",
    "        'bottom_3_items_of_week':dict_week_slow,\n",
    "        'bottom_3_items_of_day':dict_day_slow,\n",
    "        'bottom_3_items_of_month':dict_mon_slow,\n",
    "        'top_3_items_of_month':dict_mon_hot\n",
    "      }\n",
    "    }, upsert=True)\n",
    "\n",
    "    record.update_one({\n",
    "      'merchant_uid': MERCHANT_ID\n",
    "    },{\n",
    "      '$set': {\n",
    "          'weekly_quantity_sold':dict_sold_week,\n",
    "          'daily_quantity_sold':dict_sold_day,\n",
    "          'monthly_quantity_sold':dict_sold_mon\n",
    "      }\n",
    "    }, upsert=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
