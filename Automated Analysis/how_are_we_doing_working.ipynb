{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script takes the quantity of items sold over a month, removes any items that haven't been seeling hardly at all (<=5 sales) and returns the bottom 3 remaining items. These ideally are items that people like, but maybe need some promoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading payments for location with ID 19K4BY3C9DR55...\n",
      "Downloading payments for location with ID 19K4BY3C9DR55...\n",
      "Downloading payments for location with ID 19K4BY3C9DR55...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackdwyer/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:594: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/jackdwyer/anaconda/lib/python2.7/site-packages/pandas/core/generic.py:4702: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/Users/jackdwyer/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2881: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/jackdwyer/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:636: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/jackdwyer/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading payments for location with ID 2PM6V98BHFHYV...\n",
      "Downloading payments for location with ID 2PM6V98BHFHYV...\n",
      "Downloading payments for location with ID 2PM6V98BHFHYV...\n",
      "Downloading payments for location with ID 7KHHX1RQGJFXK...\n",
      "Downloading payments for location with ID 7KHHX1RQGJFXK...\n",
      "Downloading payments for location with ID 7KHHX1RQGJFXK...\n",
      "Downloading payments for location with ID 3GX7ZRNABYESR...\n",
      "Downloading payments for location with ID A4RYS03VV89MM...\n",
      "Downloading payments for location with ID 3GX7ZRNABYESR...\n",
      "Downloading payments for location with ID A4RYS03VV89MM...\n",
      "Downloading payments for location with ID 3GX7ZRNABYESR...\n",
      "Downloading payments for location with ID A4RYS03VV89MM...\n",
      "Business is very slow this hour compared to last week's business on the same day.\n",
      "Business is very slow this hour compared to the week-long average\n",
      "               name                                    id  quantity_today  \\\n",
      "11  tofu balls 3pcs  9a83c819-8098-4deb-8e01-6354b9a2c492             3.0   \n",
      "12         combo #4  a07a435c-c863-4fdc-b400-93cdcbef9066             2.0   \n",
      "14     lg 14pcs kfc  8ad6e9cc-21ce-49aa-8edb-0f0dfacf4bb2             1.0   \n",
      "\n",
      "    price_today  quantity_avg   price_avg  quant_difference  rev_difference  \n",
      "11         13.0     46.714286  202.428571        -43.714286     -189.428571  \n",
      "12         23.5     31.142857  365.928571        -29.142857     -342.428571  \n",
      "14         18.0     15.571429  280.285714        -14.571429     -262.285714  \n",
      "Downloading payments for location with ID AF8X744XSG5FE...\n",
      "Downloading payments for location with ID AF8X744XSG5FE...\n",
      "Downloading payments for location with ID AF8X744XSG5FE...\n",
      "Business is slower than usual compared to last week's business on the same day.\n",
      "Business is very slow this hour compared to the week-long average\n",
      "                             name                                    id  \\\n",
      "53          banza, chickpea penne              6WYCCMFGPATLSLJKJE4OAOKK   \n",
      "54  lotus foods millet rice ramen  6FBE9E79-AEB9-45A8-B2C7-D16BD637431E   \n",
      "55              yoconut original   F7AEDAFB-9AE4-4D6E-9681-50A0BDBC38AE   \n",
      "\n",
      "    quantity_today  price_today  quantity_avg  price_avg  quant_difference  \\\n",
      "53             1.0         3.99           8.0      31.92              -7.0   \n",
      "54             1.0         2.25           8.0      18.00              -7.0   \n",
      "55             1.0         3.49           8.0      27.92              -7.0   \n",
      "\n",
      "    rev_difference  \n",
      "53          -27.93  \n",
      "54          -15.75  \n",
      "55          -24.43  \n",
      "Downloading payments for location with ID 36XR5VCKR6AXJ...\n",
      "Downloading payments for location with ID 36XR5VCKR6AXJ...\n",
      "Downloading payments for location with ID 36XR5VCKR6AXJ...\n",
      "Downloading payments for location with ID 36XR5VCKR6AXJ...\n",
      "Downloading payments for location with ID 36XR5VCKR6AXJ...\n",
      "Downloading payments for location with ID 36XR5VCKR6AXJ...\n"
     ]
    }
   ],
   "source": [
    "#bring in relevant libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import httplib, urllib\n",
    "import re\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import httplib, urllib, json, locale\n",
    "from urlparse import urlparse\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "\n",
    "merchant_ids = []\n",
    "#connecting to our db\n",
    "connection = MongoClient(host = '')\n",
    "db=\n",
    "merchant = db['Merchants']\n",
    "for merchants in merchant.find():\n",
    "     merchant_ids.append(str(merchants['_id']))\n",
    "\n",
    "for MERCHANT_ID in merchant_ids:\n",
    "    #eventually we'll want this to be brought in externally from mongo\n",
    "\n",
    "    #object_id = 'ObjectId(\\\"' + MERCHANT_ID + '\\\")'\n",
    "    doc1 = []\n",
    "\n",
    "    for doc in merchant.find({'_id':ObjectId(MERCHANT_ID)}):\n",
    "         doc1.append(doc)\n",
    "    try:\n",
    "        access_token = doc1[0]['pos_access_token']\n",
    "\n",
    "        record=db['Reports']\n",
    "\n",
    "        access_token = str(access_token)\n",
    "    #except:\n",
    "        #pass\n",
    "\n",
    "        # Generating a payment report with the Square Connect API.\n",
    "\n",
    "\n",
    "        # Get this from your application dashboard (https://connect.squareup.com/apps)\n",
    "        # this is secret, don't share\n",
    "\n",
    "        # Standard HTTP headers for every Connect API request\n",
    "        request_headers = {'Authorization': 'Bearer ' + access_token,\n",
    "                           'Accept': 'application/json',\n",
    "                           'Content-Type': 'application/json'}\n",
    "\n",
    "        # The base URL for every Connect API request\n",
    "        connection = httplib.HTTPSConnection('connect.squareup.com')\n",
    "\n",
    "        # Uses the locale to format currency amounts correctly\n",
    "        locale.setlocale(locale.LC_ALL, 'en_US')\n",
    "\n",
    "        # Helper function to convert cent-based money amounts to dollars and cents\n",
    "        def format_money(amount):\n",
    "          return locale.currency(amount / 100.)\n",
    "\n",
    "        loc_ids = []\n",
    "        # Obtains all of the business's location IDs. Each location has its own collection of payments.\n",
    "        def get_location_ids():\n",
    "          request_path = '/v1/me/locations'\n",
    "          connection.request('GET', request_path, '', request_headers)\n",
    "          response = connection.getresponse()\n",
    "\n",
    "          # Transform the JSON array of locations into a Python list\n",
    "          locations = json.loads(response.read())\n",
    "\n",
    "          location_ids = []\n",
    "          for location in locations:\n",
    "            location_ids.append(location['id'])\n",
    "            loc_ids.append(location['id'])\n",
    "          return location_ids\n",
    "\n",
    "\n",
    "        # Downloads all of a business's payments\n",
    "        def get_payments(location_ids):\n",
    "\n",
    "        #get time right now, should be correct if computer is synced properly\n",
    "          d = datetime.utcnow()\n",
    "        #we want data for the last 12 hours (this is a safe bet, because it'll only capture the business hour's for one day inherently)\n",
    "          beg_time = datetime.utcnow() - timedelta(hours = 12)\n",
    "\n",
    "\n",
    "        #turn current time into a bunch of strings that we can add together to form link parameters below\n",
    "          year,month,day,hour,minute,second = str(d.year),str(d.month),str(d.day),str(d.hour),str(d.minute),str(d.second)\n",
    "          year_beg,month_beg,day_beg,hour_beg,minute_beg,second_beg = str(beg_time.year),str(beg_time.month),str(beg_time.day),str(beg_time.hour),str(beg_time.minute),str(beg_time.second)\n",
    "\n",
    "        #if any of our dates/times are only one digit, we add a '0' to the front to make them suitable for URL \n",
    "          if len(day) == 1:\n",
    "                day = '0'+ day\n",
    "          if len(month) == 1:\n",
    "                month = '0'+ month\n",
    "          if len(hour) == 1:\n",
    "                hour = '0' + hour\n",
    "          if len(minute) == 1:\n",
    "                minute = '0' + minute\n",
    "          if len(second) == 1:\n",
    "                second = '0' + second\n",
    "\n",
    "          if len(day_beg) == 1:\n",
    "                    day_beg = '0'+ day_beg\n",
    "          if len(month_beg) == 1:\n",
    "                    month_beg = '0'+ month_beg\n",
    "          if len(hour_beg) == 1:\n",
    "                hour_beg = '0' + hour_beg\n",
    "          if len(minute_beg) == 1:\n",
    "                minute_beg = '0' + minute_beg\n",
    "          if len(second_beg) == 1:\n",
    "                second_beg = '0' + second_beg  \n",
    "\n",
    "          parameters = urllib.urlencode({'begin_time': year_beg + '-' + month_beg + '-' + day_beg + 'T' + hour_beg + ':' + minute_beg + ':' + '00',\n",
    "                                         'end_time'  : year + '-' + month + '-' + day + 'T' + hour + ':' + minute + ':' + '00'})\n",
    "          payments = []\n",
    "\n",
    "          # For each location...\n",
    "          for location_id in location_ids:\n",
    "\n",
    "            print 'Downloading payments for location with ID ' + location_id + '...'\n",
    "\n",
    "            request_path = '/v1/' + location_id + '/payments?' + parameters\n",
    "            more_results = True\n",
    "\n",
    "            # ...as long as there are more payments to download from the location...\n",
    "            while more_results:\n",
    "\n",
    "              # ...send a GET request to /v1/LOCATION_ID/payments\n",
    "              connection.request('GET', request_path, '', request_headers)\n",
    "              response = connection.getresponse()\n",
    "\n",
    "              # Read the response body JSON into the cumulative list of results\n",
    "              payments = payments + json.loads(response.read())\n",
    "\n",
    "              # Check whether pagination information is included in a response header, indicating more results\n",
    "              pagination_header = response.getheader('link', '')\n",
    "              if \"rel='next'\" not in pagination_header:\n",
    "                more_results = False\n",
    "              else:\n",
    "\n",
    "                # Extract the next batch URL from the header.\n",
    "                #\n",
    "                # Pagination headers have the following format:\n",
    "                # <https://connect.squareup.com/v1/LOCATION_ID/payments?batch_token=BATCH_TOKEN>;rel='next'\n",
    "                # This line extracts the URL from the angle brackets surrounding it.\n",
    "                next_batch_url = urlparse(pagination_header.split('<')[1].split('>')[0])\n",
    "\n",
    "                request_path = next_batch_url.path + '?' + next_batch_url.query\n",
    "\n",
    "          # Remove potential duplicate values from the list of payments\n",
    "          seen_payment_ids = set()\n",
    "          unique_payments = []\n",
    "\n",
    "          for payment in payments:\n",
    "            if payment['id'] in seen_payment_ids:\n",
    "              continue\n",
    "            seen_payment_ids.add(payment['id'])\n",
    "            unique_payments.append(payment)\n",
    "\n",
    "          return unique_payments\n",
    "\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "\n",
    "          # Get all payments from all of the business's locations\n",
    "          payments = get_payments(get_location_ids())\n",
    "          pay_data = payments\n",
    "          # Print a sales summary report of the payments\n",
    "          #print_sales_report(payments)\n",
    "          connection.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # The base URL for every Connect API request\n",
    "        connection = httplib.HTTPSConnection('connect.squareup.com')\n",
    "\n",
    "        # Uses the locale to format currency amounts correctly\n",
    "        locale.setlocale(locale.LC_ALL, 'en_US')\n",
    "\n",
    "        # Helper function to convert cent-based money amounts to dollars and cents\n",
    "        def format_money(amount):\n",
    "          return locale.currency(amount / 100.)\n",
    "\n",
    "\n",
    "        # Obtains all of the business's location IDs. Each location has its own collection of payments.\n",
    "        def get_location_ids():\n",
    "          request_path = '/v1/me/locations'\n",
    "          connection.request('GET', request_path, '', request_headers)\n",
    "          response = connection.getresponse()\n",
    "\n",
    "          # Transform the JSON array of locations into a Python list\n",
    "          locations = json.loads(response.read())\n",
    "\n",
    "          location_ids = []\n",
    "          for location in locations:\n",
    "            location_ids.append(location['id'])\n",
    "\n",
    "          return location_ids\n",
    "\n",
    "\n",
    "        # Downloads all of a business's payments\n",
    "        def get_payments(location_ids):\n",
    "\n",
    "        #get time right now, timezone should be correct if computer is synced properly\n",
    "          d = datetime.utcnow()\n",
    "        #we want data from the last hour of today - 1 week\n",
    "          diff_time = datetime.utcnow() - timedelta(weeks = 1)\n",
    "          beg_time = diff_time - timedelta(hours = 12)\n",
    "\n",
    "\n",
    "        #turn current time into a bunch of strings that we can add together to form link parameters below\n",
    "          year_diff,month_diff,day_diff,hour_diff,minute_diff,second_diff = str(diff_time.year),str(diff_time.month),str(diff_time.day),str(diff_time.hour),str(diff_time.minute),str(diff_time.second)\n",
    "          year_beg,month_beg,day_beg,hour_beg,minute_beg,second_beg = str(beg_time.year),str(beg_time.month),str(beg_time.day),str(beg_time.hour),str(beg_time.minute),str(beg_time.second)\n",
    "\n",
    "        #if any of our dates/times are only one digit, we add a '0' to the front to make them suitable for URL \n",
    "          if len(day_diff) == 1:\n",
    "                day_diff = '0'+ day_diff\n",
    "          if len(month_diff) == 1:\n",
    "                month_diff = '0'+ month_diff\n",
    "          if len(hour_diff) == 1:\n",
    "                hour_diff = '0' + hour_diff\n",
    "          if len(minute_diff) == 1:\n",
    "                minute_diff = '0' + minute_diff\n",
    "          if len(second_diff) == 1:\n",
    "                second_diff = '0' + second_diff\n",
    "\n",
    "          if len(day_beg) == 1:\n",
    "                day_beg = '0'+ day_beg\n",
    "          if len(month_beg) == 1:\n",
    "                month_beg = '0'+ month_beg\n",
    "          if len(hour_beg) == 1:\n",
    "                hour_beg = '0' + hour_beg\n",
    "          if len(minute_beg) == 1:\n",
    "                minute_beg = '0' + minute_beg\n",
    "          if len(second_beg) == 1:\n",
    "                second_beg = '0' + second_beg\n",
    "\n",
    "          parameters = urllib.urlencode({'begin_time': year_beg + '-' + month_beg + '-' + day_beg + 'T' + hour_beg +':'+ minute_beg + ':' + '00',\n",
    "                                         'end_time'  : year_diff + '-' + month_diff + '-' + day_diff + 'T' + hour_diff + ':' + minute_diff + ':' + '00'})\n",
    "          payments = []\n",
    "\n",
    "          # For each location...\n",
    "          for location_id in location_ids:\n",
    "\n",
    "            print 'Downloading payments for location with ID ' + location_id + '...'\n",
    "\n",
    "            request_path = '/v1/' + location_id + '/payments?' + parameters\n",
    "            more_results = True\n",
    "\n",
    "            # ...as long as there are more payments to download from the location...\n",
    "            while more_results:\n",
    "\n",
    "              # ...send a GET request to /v1/LOCATION_ID/payments\n",
    "              connection.request('GET', request_path, '', request_headers)\n",
    "              response = connection.getresponse()\n",
    "\n",
    "              # Read the response body JSON into the cumulative list of results\n",
    "              payments = payments + json.loads(response.read())\n",
    "\n",
    "              # Check whether pagination information is included in a response header, indicating more results\n",
    "              pagination_header = response.getheader('link', '')\n",
    "              if \"rel='next'\" not in pagination_header:\n",
    "                more_results = False\n",
    "              else:\n",
    "\n",
    "                # Extract the next batch URL from the header.\n",
    "                #\n",
    "                # Pagination headers have the following format:\n",
    "                # <https://connect.squareup.com/v1/LOCATION_ID/payments?batch_token=BATCH_TOKEN>;rel='next'\n",
    "                # This line extracts the URL from the angle brackets surrounding it.\n",
    "                next_batch_url = urlparse(pagination_header.split('<')[1].split('>')[0])\n",
    "\n",
    "                request_path = next_batch_url.path + '?' + next_batch_url.query\n",
    "\n",
    "          # Remove potential duplicate values from the list of payments\n",
    "          seen_payment_ids = set()\n",
    "          unique_payments = []\n",
    "\n",
    "          for payment in payments:\n",
    "            if payment['id'] in seen_payment_ids:\n",
    "              continue\n",
    "            seen_payment_ids.add(payment['id'])\n",
    "            unique_payments.append(payment)\n",
    "\n",
    "          return unique_payments\n",
    "\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "\n",
    "          # Get all payments from all of the business's locations\n",
    "          payments = get_payments(get_location_ids())\n",
    "          pay_data_past = payments\n",
    "          # Print a sales summary report of the payments\n",
    "          #print_sales_report(payments)\n",
    "\n",
    "          connection.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Generating a payment report with the Square Connect API.\n",
    "\n",
    "\n",
    "        # Get this from your application dashboard (https://connect.squareup.com/apps)\n",
    "        # this is secret, don't share\n",
    "\n",
    "        # Standard HTTP headers for every Connect API request\n",
    "        request_headers = {'Authorization': 'Bearer ' + access_token,\n",
    "                           'Accept': 'application/json',\n",
    "                           'Content-Type': 'application/json'}\n",
    "\n",
    "        # The base URL for every Connect API request\n",
    "        connection = httplib.HTTPSConnection('connect.squareup.com')\n",
    "\n",
    "        # Uses the locale to format currency amounts correctly\n",
    "        locale.setlocale(locale.LC_ALL, 'en_US')\n",
    "\n",
    "        # Helper function to convert cent-based money amounts to dollars and cents\n",
    "        def format_money(amount):\n",
    "          return locale.currency(amount / 100.)\n",
    "\n",
    "\n",
    "        # Obtains all of the business's location IDs. Each location has its own collection of payments.\n",
    "        def get_location_ids():\n",
    "          request_path = '/v1/me/locations'\n",
    "          connection.request('GET', request_path, '', request_headers)\n",
    "          response = connection.getresponse()\n",
    "\n",
    "          # Transform the JSON array of locations into a Python list\n",
    "          locations = json.loads(response.read())\n",
    "\n",
    "          location_ids = []\n",
    "          for location in locations:\n",
    "            location_ids.append(location['id'])\n",
    "\n",
    "          return location_ids\n",
    "\n",
    "\n",
    "        # Downloads all of a business's payments for the last 7 days\n",
    "        def get_payments(location_ids):\n",
    "            for week_num in range(7):\n",
    "            #get time right now, timezone should be correct if computer is synced properly\n",
    "              d = datetime.utcnow()\n",
    "            #we want all the data from the last week over the last hour\n",
    "              diff_time = datetime.utcnow() - timedelta(days = int('{}'.format(week_num)))\n",
    "              beg_time = diff_time - timedelta(hours = 12)\n",
    "\n",
    "\n",
    "            #turn current time into a bunch of strings that we can add together to form link parameters below\n",
    "              year_diff,month_diff,day_diff,hour_diff,minute_diff,second_diff = str(diff_time.year),str(diff_time.month),str(diff_time.day),str(diff_time.hour),str(diff_time.minute),str(diff_time.second)\n",
    "              year_beg,month_beg,day_beg,hour_beg,minute_beg,second_beg = str(beg_time.year),str(beg_time.month),str(beg_time.day),str(beg_time.hour),str(beg_time.minute),str(beg_time.second)\n",
    "\n",
    "            #if any of our dates/times are only one digit, we add a '0' to the front to make them suitable for URL \n",
    "              if len(day_diff) == 1:\n",
    "                    day_diff = '0'+ day_diff\n",
    "              if len(month_diff) == 1:\n",
    "                    month_diff = '0'+ month_diff\n",
    "              if len(hour_diff) == 1:\n",
    "                    hour_diff = '0' + hour_diff\n",
    "              if len(minute_diff) == 1:\n",
    "                    minute_diff = '0' + minute_diff\n",
    "              if len(second_diff) == 1:\n",
    "                    second_diff = '0' + second_diff\n",
    "\n",
    "              if len(day_beg) == 1:\n",
    "                    day_beg = '0'+ day_beg\n",
    "              if len(month_beg) == 1:\n",
    "                    month_beg = '0'+ month_beg\n",
    "              if len(hour_beg) == 1:\n",
    "                    hour_beg = '0' + hour_beg\n",
    "              if len(minute_beg) == 1:\n",
    "                    minute_beg = '0' + minute_beg\n",
    "              if len(second_beg) == 1:\n",
    "                    second_beg = '0' + second_beg\n",
    "\n",
    "              parameters = urllib.urlencode({'begin_time': year_beg + '-' + month_beg + '-' + day_beg + 'T' + hour_beg +':'+ minute_beg + ':' + '00',\n",
    "                                             'end_time'  : year_diff + '-' + month_diff + '-' + day_diff + 'T' + hour_diff + ':' + minute_diff + ':' + '00'})\n",
    "              payments = []\n",
    "\n",
    "              # For each location...\n",
    "              for location_id in location_ids:\n",
    "\n",
    "                print 'Downloading payments for location with ID ' + location_id + '...'\n",
    "\n",
    "                request_path = '/v1/' + location_id + '/payments?' + parameters\n",
    "                more_results = True\n",
    "\n",
    "                # ...as long as there are more payments to download from the location...\n",
    "                while more_results:\n",
    "\n",
    "                  # ...send a GET request to /v1/LOCATION_ID/payments\n",
    "                  connection.request('GET', request_path, '', request_headers)\n",
    "                  response = connection.getresponse()\n",
    "\n",
    "                  # Read the response body JSON into the cumulative list of results\n",
    "                  payments = payments + json.loads(response.read())\n",
    "\n",
    "                  # Check whether pagination information is included in a response header, indicating more results\n",
    "                  pagination_header = response.getheader('link', '')\n",
    "                  if \"rel='next'\" not in pagination_header:\n",
    "                    more_results = False\n",
    "                  else:\n",
    "\n",
    "                    # Extract the next batch URL from the header.\n",
    "                    #\n",
    "                    # Pagination headers have the following format:\n",
    "                    # <https://connect.squareup.com/v1/LOCATION_ID/payments?batch_token=BATCH_TOKEN>;rel='next'\n",
    "                    # This line extracts the URL from the angle brackets surrounding it.\n",
    "                    next_batch_url = urlparse(pagination_header.split('<')[1].split('>')[0])\n",
    "\n",
    "                    request_path = next_batch_url.path + '?' + next_batch_url.query\n",
    "\n",
    "              # Remove potential duplicate values from the list of payments\n",
    "              seen_payment_ids = set()\n",
    "              unique_payments = []\n",
    "              avg_list = []\n",
    "              for payment in payments:\n",
    "                if payment['id'] in seen_payment_ids:\n",
    "                  continue\n",
    "                seen_payment_ids.add(payment['id'])\n",
    "                unique_payments.append(payment)\n",
    "                avg_list.append(unique_payments)\n",
    "              return avg_list\n",
    "\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "\n",
    "          # Get all 2015 payments from all of the business's locations\n",
    "          payments = get_payments(get_location_ids())\n",
    "          #pay_data_avg is a list of jsons in this case\n",
    "          pay_data_avg = payments\n",
    "          # Print a sales summary report of the payments\n",
    "          #print_sales_report(payments)\n",
    "\n",
    "          connection.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #list items, list payments\n",
    "\n",
    "        # All requests to the Square Connect API require an access token in an\n",
    "        # Authorization header. Specify your application's personal access token here\n",
    "        # (available from https://connect.squareup.com/apps)\n",
    "\n",
    "        # In addition to an Authorization header, requests to the Connect API should\n",
    "        # include the indicated Accept and Content-Type headers.\n",
    "\n",
    "\n",
    "        # Send a GET request to the ListLocations endpoint and obtain the response.\n",
    "        connection = httplib.HTTPSConnection('connect.squareup.com')\n",
    "        request_path = '/v1/'+loc_ids[0]+'/items'\n",
    "        connection.request('GET', request_path, '', request_headers)\n",
    "        response = connection.getresponse()\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the returned JSON body into an array of locations you can work with.\n",
    "        items_data = json.loads(response.read())\n",
    "\n",
    "        # Pretty-print the locations array.\n",
    "        #locations = json.dumps(locations, indent=2, separators=(',',': '))\n",
    "\n",
    "\n",
    "        #making empty lists to fill with JSON pulled data\n",
    "        name_list = []\n",
    "        quant_list = []\n",
    "        quant_id_list = []\n",
    "        time_list = []\n",
    "        #pulling in item name,quantity purchased for the day, id of item, and time of purchase\n",
    "        for i in pay_data:\n",
    "            for n in range(10):\n",
    "                #all of our entries should be complete every time, if not, we'll add a nan\n",
    "                try:\n",
    "                    name_list.append(i['itemizations'][n]['name'])\n",
    "                except:\n",
    "                    name_list.append(np.nan)\n",
    "                try:\n",
    "                    quant_list.append(str(i['itemizations'][n]['quantity']))\n",
    "                except:\n",
    "                    quant_list.append(0)\n",
    "                try:\n",
    "                    quant_id_list.append(i['itemizations'][n]['item_detail']['item_id'])\n",
    "                except:\n",
    "                    quant_id_list.append(np.nan)\n",
    "                try:\n",
    "                    time_list.append(i['created_at'])\n",
    "                except:\n",
    "                    time_list.append(np.nan)\n",
    "\n",
    "\n",
    "\n",
    "        name_list_past = []\n",
    "        quant_list_past = []\n",
    "        quant_id_list_past = []\n",
    "        time_list_past = []\n",
    "        #pulling in item name,quantity purchased for the day, id of item, and time of purchase\n",
    "        for i in pay_data_past:\n",
    "            for n in range(10):\n",
    "                #all of our entries should be complete every time, if not, we'll add a nan\n",
    "                try:\n",
    "                    name_list_past.append(i['itemizations'][n]['name'])\n",
    "                except:\n",
    "                    name_list_past.append(np.nan)\n",
    "                try:\n",
    "                    quant_list_past.append(str(i['itemizations'][n]['quantity']))\n",
    "                except:\n",
    "                    quant_list_past.append(0)\n",
    "                try:\n",
    "                    quant_id_list_past.append(i['itemizations'][n]['item_detail']['item_id'])\n",
    "                except:\n",
    "                    quant_id_list_past.append(np.nan)\n",
    "                try:\n",
    "                    time_list_past.append(i['created_at'])\n",
    "                except:\n",
    "                    time_list_past.append(np.nan)\n",
    "\n",
    "        name_list_avg = []\n",
    "        quant_list_avg = []\n",
    "        quant_id_list_avg = []\n",
    "        time_list_avg = []\n",
    "        #pulling in item name,quantity purchased for the day, id of item, and time of purchase\n",
    "        #since it's a list, we need to iterate over each element of the list as well as the actual json files\n",
    "        for el in range(len(pay_data_avg)):  \n",
    "            for i in pay_data_avg[el]:\n",
    "                for n in range(10):\n",
    "                    #all of our entries should be complete every time, if not, we'll add a nan\n",
    "                    try:\n",
    "                        name_list_avg.append(i['itemizations'][n]['name'])\n",
    "                    except:\n",
    "                        name_list_avg.append(np.nan)\n",
    "                    try:\n",
    "                        quant_list_avg.append(str(i['itemizations'][n]['quantity']))\n",
    "                    except:\n",
    "                        quant_list_avg.append(0)\n",
    "                    try:\n",
    "                        quant_id_list_avg.append(i['itemizations'][n]['item_detail']['item_id'])\n",
    "                    except:\n",
    "                        quant_id_list_avg.append(np.nan)\n",
    "                    try:\n",
    "                        time_list_avg.append(i['created_at'])\n",
    "                    except:\n",
    "                        time_list_avg.append(np.nan)\n",
    "        \n",
    "        \n",
    "        #if our quantity for today and 7 days ago today is 0, the restaurant likely is not open, so we won't analyze\n",
    "        #if np.sum(quant_list) == 0 and np.sum(quant_list_past) == 0:\n",
    "         #   print (np.sum(quant_list), np.sum(quant_list_past))\n",
    "          #  continue\n",
    "\n",
    "        id_list = []\n",
    "        url_list = []\n",
    "        names_list = []\n",
    "        price_list = []\n",
    "        #pulling in item, id, price of item, and image url\n",
    "        for m in items_data:\n",
    "            #try and pull in data, if it doesn't exist for the instance, add a nan to be handled later\n",
    "            try:      \n",
    "                names_list.append(m['name'])\n",
    "            except:\n",
    "                names_list.append(np.nan)\n",
    "            try:\n",
    "                id_list.append(m['id'])\n",
    "            except:\n",
    "                id_list.append(np.nan)\n",
    "            try:\n",
    "                url_list.append(m['master_image']['url'])\n",
    "            except:\n",
    "                url_list.append(np.nan)\n",
    "            try:\n",
    "                price_list.append(m['variations'][0]['price_money']['amount'])\n",
    "            except:\n",
    "                price_list.append(np.nan)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #create data frame for each item on the menu and all the relevant information about it\n",
    "        df2 = pd.DataFrame({'id':id_list,'url':url_list, 'name':names_list, 'price':price_list})\n",
    "        df2.price = df2.price/100\n",
    "        df2_name_url= df2[['name','url']]\n",
    "        df2_name_url.url[pd.isnull(df2_name_url.url)] = ''\n",
    "\n",
    "        #the items without a url are not typical menu items, so we get rid of those entries.\n",
    "        df2 = df2.dropna(subset = ['name']).reset_index()\n",
    "\n",
    "\n",
    "        #Compile the payment data into a dataframe\n",
    "        df_day = pd.DataFrame({'quantity':quant_list,'id':quant_id_list,'item_name':name_list,'time_of_sale':time_list})\n",
    "        df_past = pd.DataFrame({'quantity':quant_list_past,'id':quant_id_list_past,'item_name':name_list_past,'time_of_sale':time_list_past})\n",
    "        df_avg = pd.DataFrame({'quantity':quant_list_avg,'id':quant_id_list_avg,'item_name':name_list_avg,'time_of_sale':time_list_avg})\n",
    "\n",
    "\n",
    "        #mergethe dataframes on ID, but maintain\n",
    "        fin_df_day = df_day.merge(df2, on = 'id', how = 'outer')\n",
    "        fin_df_past = df_past.merge(df2, on = 'id', how = 'outer')\n",
    "        fin_df_avg = df_avg.merge(df2,on = 'id', how = 'outer')\n",
    "\n",
    "        #we aren't interest in all of the columns from the original two dataframe, so we take the ones we want\n",
    "        fin_df_day = fin_df_day[['name','quantity','id','price','url','time_of_sale']]\n",
    "        fin_df_past = fin_df_past[['name','quantity','id','price','url','time_of_sale']]\n",
    "        fin_df_avg = fin_df_avg[['name','quantity','id','price','url','time_of_sale']]\n",
    "\n",
    "        #we are turning quantity into a numeric so we can aggregate it to get totala quantity sold in our group by below\n",
    "        fin_df_day.quantity = pd.to_numeric(fin_df_day.loc[:,'quantity'])\n",
    "        fin_df_past.quantity = pd.to_numeric(fin_df_past.loc[:,'quantity'])\n",
    "        fin_df_avg.quantity = pd.to_numeric(fin_df_avg.loc[:,'quantity'])\n",
    "\n",
    "\n",
    "        #groupby quantity to get total sold for each product\n",
    "        fin_df_day = fin_df_day.groupby(['name','id']).sum().reset_index().sort_values('quantity', ascending = False)\n",
    "        fin_df_past = fin_df_past.groupby(['name','id']).sum().reset_index().sort_values('quantity', ascending = False)\n",
    "        fin_df_avg = fin_df_avg.groupby(['name','id']).sum().reset_index().sort_values('quantity', ascending = False)\n",
    "        \n",
    "\n",
    "        fin_df_day = fin_df_day.reset_index(drop=True)\n",
    "        fin_df_past = fin_df_past.reset_index(drop=True)\n",
    "        fin_df_avg = fin_df_avg.reset_index(drop=True)\n",
    "\n",
    "        #bring together our today df and the last week one so we can compare\n",
    "        fin_df = fin_df_day.merge(fin_df_past, on = ['name','id'], suffixes = ['_today','_week_ago'])\n",
    "\n",
    "        #turn our nulls into 0's so we can use them in math operations\n",
    "        fin_df.quantity_today[pd.isnull(fin_df.quantity_today)] = 0\n",
    "        fin_df.quantity_week_ago[pd.isnull(fin_df.quantity_week_ago)] = 0\n",
    "\n",
    "        stop_words = ['drink','bag','gift','card','sauce','dip','soda','coke','pepsi','sprite','cola','beer','beverage','extra','crumbles']\n",
    "\n",
    "\n",
    "        #get rid of stop words\n",
    "        fin_df.name = fin_df.name.str.lower()\n",
    "\n",
    "        indexes = [True]*len(fin_df.name)\n",
    "\n",
    "        for i, name in enumerate(fin_df.name):\n",
    "            for word in stop_words:\n",
    "                if word in name:\n",
    "                    indexes[i]=False\n",
    "\n",
    "\n",
    "        fin_df = fin_df[indexes]\n",
    "\n",
    "\n",
    "        fin_df.name = fin_df.name.str.lower()\n",
    "\n",
    "        indexes = [True]*len(fin_df.name)\n",
    "\n",
    "        for i, name in enumerate(fin_df.name):\n",
    "            for word in stop_words:\n",
    "                if word in name:\n",
    "                    indexes[i]=False\n",
    "\n",
    "\n",
    "        fin_df = fin_df[indexes]\n",
    "        #merge day and avg data to make the comparison possible\n",
    "        fin_df_avg = fin_df_day.merge(fin_df_avg, on = ['name','id'], suffixes = ['_today','_avg'])\n",
    "        #filter out all items that haven't sold at least 5 products\n",
    "        fin_df_avg = fin_df_avg[fin_df_avg.quantity_avg >= 5]\n",
    "        \n",
    "        fin_df_avg.quantity_today[pd.isnull(fin_df_avg.quantity_today)] = ''\n",
    "        fin_df_avg.quantity_avg[pd.isnull(fin_df_avg.quantity_avg)] = 0\n",
    "\n",
    "\n",
    "        fin_df_avg.name = fin_df_avg.name.str.lower()\n",
    "\n",
    "        indexes = [True]*len(fin_df_avg.name)\n",
    "\n",
    "        for i, name in enumerate(fin_df_avg.name):\n",
    "            for word in stop_words:\n",
    "                if word in name:\n",
    "                    indexes[i]=False\n",
    "        \n",
    "        fin_df_avg.quantity_avg = fin_df_avg.quantity_avg[:]/7\n",
    "        fin_df_avg.price_avg = fin_df_avg.price_avg.divide(7)\n",
    "\n",
    "        fin_df_avg = fin_df_avg[indexes]\n",
    "        #difference the quantity and revenue of today and a week ago, this will give us a differential that has meaning\n",
    "        fin_df['quant_difference'] = (fin_df.quantity_today-fin_df.quantity_week_ago)\n",
    "        fin_df['rev_difference'] = (fin_df.price_today-fin_df.price_week_ago)\n",
    "\n",
    "        fin_df_avg['quant_difference'] = (fin_df_avg.quantity_today-fin_df_avg.quantity_avg)\n",
    "        fin_df_avg['rev_difference'] = (fin_df_avg.price_today-fin_df_avg.price_avg)\n",
    "\n",
    "        if np.sum(fin_df.price_today) != 0 or np.sum(fin_df.price_week_ago) != 0:\n",
    "            #let them know how they are doing with total sales figures (revenue based)\n",
    "            if np.sum(fin_df.rev_difference) > -20 and np.sum(fin_df.rev_difference) < 100:\n",
    "                print('Business as usual compared to last week\\'s business on the same day.')\n",
    "            elif np.sum(fin_df.rev_difference) > 100:\n",
    "                print('You are doing very well this hour compared to last week\\'s business on the same day.')\n",
    "            elif np.sum(fin_df.rev_difference) < -20 and np.sum(fin_df.rev_difference) > -100:\n",
    "                print('Business is slower than usual compared to last week\\'s business on the same day.')\n",
    "            else:\n",
    "                print('Business is very slow this hour compared to last week\\'s business on the same day.')\n",
    "\n",
    "\n",
    "        if np.sum(fin_df.price_today) != 0 or np.sum(fin_df.price_avg) != 0:\n",
    "            if np.sum(fin_df_avg.rev_difference) > -20 and np.sum(fin_df_avg.rev_difference) < 100:\n",
    "                print('Business as usual compared to the weekly average.')\n",
    "            elif np.sum(fin_df_avg.rev_difference) > 100:\n",
    "                print('You are doing very well this hour compared to the week-long average')\n",
    "            elif np.sum(fin_df_avg.rev_difference) < -20 and np.sum(fin_df_avg.rev_difference) > -100:\n",
    "                print('Business is slower than usual compared to the week-long average')\n",
    "            else:\n",
    "                print('Business is very slow this hour compared to the week-long average')\n",
    "\n",
    "        #Let them know which item has the biggest sales differential    \n",
    "        #if np.sum(fin_df.quantity_today) != 0 or np.sum(fin_df.quantity_week_ago) != 0:\n",
    "            #print('This item has the biggest negative differential from last week: %s'%fin_df.name.iloc[np.argmax(fin_df.quant_difference)])\n",
    "        #if np.sum(fin_df.quantity_today) != 0 or np.sum(fin_df.quantity_avg) != 0:\n",
    "        #    print('This item has the biggest negative differential from the average for this hour: %s'%fin_df_avg.name.iloc[np.argmax(fin_df_avg.quant_difference)])\n",
    "        #except:\n",
    "        #    pass\n",
    "        #fin_df_avg[np.argpartition(fin_df_avg.quantity_avg, 3)]\n",
    "        ##slow_today = fin_df.nsmallest(3,'quantity_today')\n",
    "        #slow_today = fin_df[np.argmin(fin_df.quantity_today, axis = 1)]\n",
    "\n",
    "        #slow_past = fin_df[np.argmin(fin_df.quantity_week_ago, axis = 1)]\n",
    "        \n",
    "        #slow_avg = fin_df_avg[np.argmin(fin_df_avg.quantity_avg)]\n",
    "\n",
    "        rev_avg = fin_df_avg.price_avg\n",
    "        rev_today = fin_df_avg.price_today\n",
    "        rev_week_ago = fin_df.price_week_ago\n",
    "\n",
    "        fin_df_avg_bottom_3 = fin_df_avg.iloc[-3:,:]\n",
    "\n",
    "        #fin_df_avg_bottom_3 = fin_df_avg.merge(df2_name_url,on = 'name', how = 'outer')\n",
    "        #print rev_today\n",
    "        #print rev_week_ago\n",
    "        bottom_3_dict = fin_df_avg_bottom_3.to_dict('records')\n",
    "        #slow_today\n",
    "        #slow_past\n",
    "        #print(slow_avg.head())\n",
    "\n",
    "        record.update_many({\n",
    "              'merchant_uid': MERCHANT_ID\n",
    "                },{\n",
    "                '$set': {\n",
    "                    'Need_to_Pump_products':bottom_3_dict\n",
    "                   \n",
    "                      }\n",
    "                }, upsert=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
