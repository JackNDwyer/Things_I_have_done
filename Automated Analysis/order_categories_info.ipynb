{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#bring in relevant libraries\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import httplib, urllib, json, locale\n",
    "from urlparse import urlparse\n",
    "import operator\n",
    "import bson\n",
    "from collections import OrderedDict\n",
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading payments for location with ID 19K4BY3C9DR55...\n",
      "Downloading payments for location with ID 2PM6V98BHFHYV...\n",
      "Downloading payments for location with ID 7KHHX1RQGJFXK...\n",
      "0\n",
      "{'charcuterie': {'charcuterie': 5.0, 'grocery': 6.0, 'wine': 3.0, 'bread': 2.0}, '': {'price': 12.975, 'category_id': ''}, 'cheese': {'price': 21.990000000000006, 'category_id': u'EEDBD7E8-F5CC-4AF0-8366-E2E3A6F85207'}, 'catering': [{'grocery': 3.0, 'catering': 3.0, 'bread': 3.0}, {'price': 55.39300000000001, 'category_id': u'KKKLIQ2O7OHTIHHAWY3BHB5Y'}], 'happy hour': [{'grocery': 2.0, 'wine': 2.0, 'sandwiches': 6.0, 'drinks': 6.0}, {'price': 18.331666666666667, 'category_id': u'JRWLHZCLQVRB22JSROEMXSFG'}], 'breakfast': [{}, {'price': 3.5081250000000006, 'category_id': u'94B075F2-F7BD-4793-8006-B2740AB58C86'}], 'salads': [{'grocery': 2.0, 'wine': 2.0, 'sandwiches': 2.0, 'grilled cheeses': 2.0, 'drinks': 4.0}, {'price': 10.49, 'category_id': u'3592C730-C045-4A8F-8ACB-CD7DB8665399'}], 'sides': [{'grocery': 6.0, 'sandwiches': 89.0, 'grilled cheeses': 14.0, 'drinks': 6.0}, {'price': 2.25, 'category_id': u'AE240C9C-5D54-4FFB-B8AA-4D133BBEFF35'}], 'drinks': [{'happy hour': 3.0, 'salads': 4.0, 'sandwiches': 16.0, 'grilled cheeses': 14.0, 'sides': 6.0}, {'price': 2.25, 'category_id': u'54A22AD1-6B53-465A-8879-19CC6B916AD0'}], 'grocery': {'charcuterie': 3.0, 'grocery': 25.0, 'catering': 3.0, 'happy hour': 2.0, 'sandwiches': 3.0, 'grilled cheeses': 3.0, 'wine': 5.0, 'salads': 2.0, 'sides': 6.0, 'bread': 7.0}, 'private event': {'price': 350.0, 'category_id': u'RSPWCGKA4LGA2P3NUFGRXCI4'}, 'soup': [{'price': 6.99, 'category_id': u'75DG3JDNQB5YZJLVHPEGJYML'}], 'sandwiches': [{'grocery': 3.0, 'happy hour': 3.0, 'grilled cheeses': 3.0, 'salads': 2.0, 'sides': 87.0, 'drinks': 16.0}, {'price': 9.99, 'category_id': u'8D3680D3-86D3-4742-8DEB-517E09C7FF47'}], 'grilled cheeses': [{'sides': 13.0, 'grocery': 3.0, 'salads': 2.0, 'sandwiches': 6.0, 'drinks': 8.0}, {'price': 7.2425, 'category_id': u'DD87D38D-3F82-4739-AD2B-58713AA6C649'}], 'bread': [{'charcuterie': 2.0, 'grocery': 7.0, 'catering': 3.0}, {'price': 4.492, 'category_id': u'844215A2-4E93-4582-B071-62843B27C8F1'}], 'wine': [{'charcuterie': 3.0, 'grocery': 8.0, 'salads': 2.0, 'happy hour': 2.0}, {'price': 25.451538461538487, 'category_id': u'A88EB228-FABA-4067-A6B4-CD80F49C2BEA'}]}\n",
      "0\n",
      "Downloading payments for location with ID A4RYS03VV89MM...\n",
      "Downloading payments for location with ID AF8X744XSG5FE...\n",
      "0\n",
      "{'charcuterie': {'charcuterie': 5.0, 'grocery': 6.0, 'wine': 3.0, 'bread': 2.0}, '': {'price': 12.975, 'category_id': ''}, 'cheese': {'price': 21.990000000000006, 'category_id': u'EEDBD7E8-F5CC-4AF0-8366-E2E3A6F85207'}, 'catering': [{'grocery': 3.0, 'catering': 3.0, 'bread': 3.0}, {'price': 55.39300000000001, 'category_id': u'KKKLIQ2O7OHTIHHAWY3BHB5Y'}], 'happy hour': [{'grocery': 2.0, 'wine': 2.0, 'sandwiches': 6.0, 'drinks': 6.0}, {'price': 18.331666666666667, 'category_id': u'JRWLHZCLQVRB22JSROEMXSFG'}], 'breakfast': [{}, {'price': 3.5081250000000006, 'category_id': u'94B075F2-F7BD-4793-8006-B2740AB58C86'}], 'salads': [{'grocery': 2.0, 'wine': 2.0, 'sandwiches': 2.0, 'grilled cheeses': 2.0, 'drinks': 4.0}, {'price': 10.49, 'category_id': u'3592C730-C045-4A8F-8ACB-CD7DB8665399'}], 'sides': [{'grocery': 6.0, 'sandwiches': 89.0, 'grilled cheeses': 14.0, 'drinks': 6.0}, {'price': 2.25, 'category_id': u'AE240C9C-5D54-4FFB-B8AA-4D133BBEFF35'}], 'drinks': [{'happy hour': 3.0, 'salads': 4.0, 'sandwiches': 16.0, 'grilled cheeses': 14.0, 'sides': 6.0}, {'price': 2.25, 'category_id': u'54A22AD1-6B53-465A-8879-19CC6B916AD0'}], 'grocery': {'charcuterie': 3.0, 'grocery': 25.0, 'catering': 3.0, 'happy hour': 2.0, 'sandwiches': 3.0, 'grilled cheeses': 3.0, 'wine': 5.0, 'salads': 2.0, 'sides': 6.0, 'bread': 7.0}, 'private event': {'price': 350.0, 'category_id': u'RSPWCGKA4LGA2P3NUFGRXCI4'}, 'soup': [{'price': 6.99, 'category_id': u'75DG3JDNQB5YZJLVHPEGJYML'}], 'sandwiches': [{'grocery': 3.0, 'happy hour': 3.0, 'grilled cheeses': 3.0, 'salads': 2.0, 'sides': 87.0, 'drinks': 16.0}, {'price': 9.99, 'category_id': u'8D3680D3-86D3-4742-8DEB-517E09C7FF47'}], 'grilled cheeses': [{'sides': 13.0, 'grocery': 3.0, 'salads': 2.0, 'sandwiches': 6.0, 'drinks': 8.0}, {'price': 7.2425, 'category_id': u'DD87D38D-3F82-4739-AD2B-58713AA6C649'}], 'bread': [{'charcuterie': 2.0, 'grocery': 7.0, 'catering': 3.0}, {'price': 4.492, 'category_id': u'844215A2-4E93-4582-B071-62843B27C8F1'}], 'wine': [{'charcuterie': 3.0, 'grocery': 8.0, 'salads': 2.0, 'happy hour': 2.0}, {'price': 25.451538461538487, 'category_id': u'A88EB228-FABA-4067-A6B4-CD80F49C2BEA'}]}\n",
      "0\n",
      "Downloading payments for location with ID DRG6Z69RW5N00...\n",
      "0\n",
      "{'charcuterie': {'charcuterie': 5.0, 'grocery': 6.0, 'wine': 3.0, 'bread': 2.0}, '': {'price': 12.975, 'category_id': ''}, 'cheese': {'price': 21.990000000000006, 'category_id': u'EEDBD7E8-F5CC-4AF0-8366-E2E3A6F85207'}, 'catering': [{'grocery': 3.0, 'catering': 3.0, 'bread': 3.0}, {'price': 55.39300000000001, 'category_id': u'KKKLIQ2O7OHTIHHAWY3BHB5Y'}], 'happy hour': [{'grocery': 2.0, 'wine': 2.0, 'sandwiches': 6.0, 'drinks': 6.0}, {'price': 18.331666666666667, 'category_id': u'JRWLHZCLQVRB22JSROEMXSFG'}], 'breakfast': [{}, {'price': 3.5081250000000006, 'category_id': u'94B075F2-F7BD-4793-8006-B2740AB58C86'}], 'salads': [{'grocery': 2.0, 'wine': 2.0, 'sandwiches': 2.0, 'grilled cheeses': 2.0, 'drinks': 4.0}, {'price': 10.49, 'category_id': u'3592C730-C045-4A8F-8ACB-CD7DB8665399'}], 'sides': [{'grocery': 6.0, 'sandwiches': 89.0, 'grilled cheeses': 14.0, 'drinks': 6.0}, {'price': 2.25, 'category_id': u'AE240C9C-5D54-4FFB-B8AA-4D133BBEFF35'}], 'drinks': [{'happy hour': 3.0, 'salads': 4.0, 'sandwiches': 16.0, 'grilled cheeses': 14.0, 'sides': 6.0}, {'price': 2.25, 'category_id': u'54A22AD1-6B53-465A-8879-19CC6B916AD0'}], 'grocery': {'charcuterie': 3.0, 'grocery': 25.0, 'catering': 3.0, 'happy hour': 2.0, 'sandwiches': 3.0, 'grilled cheeses': 3.0, 'wine': 5.0, 'salads': 2.0, 'sides': 6.0, 'bread': 7.0}, 'private event': {'price': 350.0, 'category_id': u'RSPWCGKA4LGA2P3NUFGRXCI4'}, 'soup': [{'price': 6.99, 'category_id': u'75DG3JDNQB5YZJLVHPEGJYML'}], 'sandwiches': [{'grocery': 3.0, 'happy hour': 3.0, 'grilled cheeses': 3.0, 'salads': 2.0, 'sides': 87.0, 'drinks': 16.0}, {'price': 9.99, 'category_id': u'8D3680D3-86D3-4742-8DEB-517E09C7FF47'}], 'grilled cheeses': [{'sides': 13.0, 'grocery': 3.0, 'salads': 2.0, 'sandwiches': 6.0, 'drinks': 8.0}, {'price': 7.2425, 'category_id': u'DD87D38D-3F82-4739-AD2B-58713AA6C649'}], 'bread': [{'charcuterie': 2.0, 'grocery': 7.0, 'catering': 3.0}, {'price': 4.492, 'category_id': u'844215A2-4E93-4582-B071-62843B27C8F1'}], 'wine': [{'charcuterie': 3.0, 'grocery': 8.0, 'salads': 2.0, 'happy hour': 2.0}, {'price': 25.451538461538487, 'category_id': u'A88EB228-FABA-4067-A6B4-CD80F49C2BEA'}]}\n",
      "0\n",
      "Downloading payments for location with ID 36XR5VCKR6AXJ...\n",
      "Downloading payments for location with ID 36XR5VCKR6AXJ...\n",
      "Downloading payments for location with ID 5ZD2B6RBEA90V...\n",
      "Downloading payments for location with ID 525YR12PVCRX4...\n"
     ]
    }
   ],
   "source": [
    "merchant_ids = []\n",
    "#connecting to our db\n",
    "connection = MongoClient(host = '')\n",
    "db=\n",
    "merchant = db['Merchants']\n",
    "items = db['Items']\n",
    "for merchants in merchant.find():\n",
    "     merchant_ids.append(str(merchants['_id']))\n",
    "for MERCHANT_ID in merchant_ids:\n",
    "    locale.setlocale(locale.LC_ALL, 'en_US')\n",
    "    doc1 = []\n",
    "    for doc in merchant.find({'_id':ObjectId(MERCHANT_ID)}):\n",
    "        doc1.append(doc)\n",
    "    try:\n",
    "        access_token = doc1[0]['pos_access_token']\n",
    "        location_id = doc1[0]['location_ids'][0]        \n",
    "    except:\n",
    "        continue\n",
    "    request_headers = {'Authorization': 'Bearer ' + access_token,\n",
    "                           'Accept': 'application/json',\n",
    "                           'Content-Type': 'application/json'}    \n",
    "    connection = httplib.HTTPSConnection('connect.squareup.com')\n",
    "\n",
    "    def format_money(amount):\n",
    "        return locale.currency(amount / 100.)\n",
    "\n",
    "\n",
    "        # Obtains all of the business's location IDs. Each location has its own collection of payments.\n",
    "    def get_location_ids():\n",
    "        request_path = '/v1/me/locations'\n",
    "        connection.request('GET', request_path, '', request_headers)\n",
    "        response = connection.getresponse()\n",
    "\n",
    "        # Transform the JSON array of locations into a Python list\n",
    "        locations = json.loads(response.read())\n",
    "\n",
    "        location_ids = []\n",
    "        for location in locations:\n",
    "            if location is None:\n",
    "                continue\n",
    "        location_ids.append(location['id'])\n",
    "\n",
    "        return location_ids    \n",
    "\n",
    "    def get_payments_month(location_ids):\n",
    "\n",
    "        #get time right now, timezone should be correct if computer is synced properly\n",
    "        d = datetime.utcnow()\n",
    "        diff_time = datetime.utcnow() - timedelta(days = 1)\n",
    "\n",
    "\n",
    "          #time_now\n",
    "          # Make sure to URL-encode all parameters\n",
    "\n",
    "        #turn current time into a bunch of strings that we can add together to form link parameters below\n",
    "        year,month,day,hour,minute,second = str(d.year),str(d.month),str(d.day),str(d.hour),str(d.minute),str(d.second)\n",
    "        year_diff,month_diff,day_diff,hour_diff,minute_diff,second_diff = str(diff_time.year),str(diff_time.month),str(diff_time.day),str(diff_time.hour),str(diff_time.minute),str(diff_time.second)\n",
    "\n",
    "        #if any of our dates/times are only one digit, we add a '0' to the front to make them suitable for URL \n",
    "        if len(day) == 1:\n",
    "            day = '0'+ day\n",
    "        if len(month) == 1:\n",
    "            month = '0'+ month\n",
    "        if len(hour) == 1:\n",
    "            hour = '0' + hour\n",
    "        if len(minute) == 1:\n",
    "            minute = '0' + minute\n",
    "        if len(second) == 1:\n",
    "            second = '0' + second\n",
    "\n",
    "        if len(day_diff) == 1:\n",
    "            day_diff = '0'+ day_diff\n",
    "        if len(month_diff) == 1:\n",
    "            month_diff = '0'+ month_diff\n",
    "        if len(hour_diff)==1:\n",
    "            hour_diff = '0' + hour_diff\n",
    "\n",
    "        parameters = urllib.urlencode({'begin_time': year_diff + '-' + month_diff + '-' + day_diff + 'T'+ hour_diff + ':00:00',\n",
    "                                         'end_time': year + '-' + month + '-' + day + 'T' + hour + ':00:00'})\n",
    "        payments = []\n",
    "\n",
    "          # For each location...\n",
    "        for location_id in location_ids:\n",
    "\n",
    "            print 'Downloading payments for location with ID ' + location_id + '...'\n",
    "\n",
    "            request_path = '/v1/' + location_id + '/payments?' + parameters\n",
    "            more_results = True\n",
    "\n",
    "            # ...as long as there are more payments to download from the location...\n",
    "            while more_results:\n",
    "\n",
    "              # ...send a GET request to /v1/LOCATION_ID/payments\n",
    "                connection.request('GET', request_path, '', request_headers)\n",
    "                response = connection.getresponse()\n",
    "\n",
    "              # Read the response body JSON into the cumulative list of results\n",
    "                payments = payments + json.loads(response.read())\n",
    "\n",
    "              # Check whether pagination information is included in a response header, indicating more results\n",
    "                pagination_header = response.getheader('link', '')\n",
    "                if \"rel='next'\" not in pagination_header:\n",
    "                    more_results = False\n",
    "                else:\n",
    "\n",
    "                # Extract the next batch URL from the header.\n",
    "                #\n",
    "                # Pagination headers have the following format:\n",
    "                # <https://connect.squareup.com/v1/LOCATION_ID/payments?batch_token=BATCH_TOKEN>;rel='next'\n",
    "                # This line extracts the URL from the angle brackets surrounding it.\n",
    "                    next_batch_url = urlparse(pagination_header.split('<')[1].split('>')[0])\n",
    "\n",
    "                    request_path = next_batch_url.path + '?' + next_batch_url.query\n",
    "\n",
    "          # Remove potential duplicate values from the list of payments\n",
    "        seen_payment_ids = set()\n",
    "        unique_payments = []\n",
    "\n",
    "        for payment in payments:\n",
    "            try:\n",
    "                if payment['id'] in seen_payment_ids:\n",
    "                    coprice_ntinue\n",
    "                seen_payment_ids.add(payment['id'])\n",
    "                unique_payments.append(payment)\n",
    "            except:\n",
    "                continue\n",
    "        return unique_payments\n",
    "\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        # Get all 2015 payments from all of the business's locations\n",
    "\n",
    "        payments = get_payments_month(get_location_ids())\n",
    "        pay_data_month = payments\n",
    "        if len(pay_data_month) == 0:\n",
    "            continue\n",
    "        # Print a sales summary report of the payments\n",
    "        #print_sales_report(payments)\n",
    "    try:\n",
    "        #connecting to the items df, this will allow us to bring in price and category id\n",
    "        connection = httplib.HTTPSConnection('connect.squareup.com')\n",
    "        request_path = '/v1/'+location_id+'/items'\n",
    "        connection.request('GET', request_path, '', request_headers)\n",
    "        response = connection.getresponse()\n",
    "        # Convert the returned JSON body into an array of locations you can work with.\n",
    "        items_data = json.loads(response.read())\n",
    "\n",
    "        #initialize empty lists that will be populated with data from our JSON api file\n",
    "        cat_name = []\n",
    "        cat_id = []\n",
    "        item_name = []\n",
    "        price_list = []\n",
    "\n",
    "        for q in items_data:\n",
    "            try:\n",
    "                cat_name.append(str(q['category']['name']).lower())\n",
    "            except:\n",
    "                cat_name.append('')\n",
    "            try:\n",
    "                cat_id.append(q['category_id'])\n",
    "            except:\n",
    "                cat_id.append('')\n",
    "            try:\n",
    "                item_name.append(q['name'])\n",
    "            except:\n",
    "                item_name.append('')\n",
    "            try:\n",
    "                price_list.append(float(q['variations'][0]['price_money']['amount'])/100)\n",
    "            except:\n",
    "                price_list.append(np.nan)\n",
    "\n",
    "        #make a dataframe from the lists\n",
    "        df2 = pd.DataFrame({'category_id':cat_id,'cat_name':cat_name, 'name':item_name, 'price':price_list})\n",
    "        #group by to get average price for each category\n",
    "        avg_price_df = df2.groupby(['category_id','cat_name']).mean()\n",
    "        #convert our dataframe to a dictionary that has the important inifo in a {'cat_name':[price,cat_id]} format\n",
    "        price_id_dict = avg_price_df.reset_index().set_index('cat_name').to_dict('index')\n",
    "        \n",
    "        #make empty list of various grouped categories\n",
    "        categories_together = []\n",
    "        #for every entry in our payments data, we pull the category name by iterating through the itemizations\n",
    "        for q in pay_data_month:\n",
    "            cat_name_mon = []\n",
    "            for i in q['itemizations']:\n",
    "                cat_name_mon.append(str(i['item_detail']['category_name']).lower())\n",
    "            #append each category into a list from the transaction, then add this list to a greater list\n",
    "            categories_together.append(cat_name_mon)\n",
    "\n",
    "\n",
    "        #for each sublist, we get rid of blank entries, make a set out of them (remove duplicates), and ensure that only orders\n",
    "        #of length 2 or greater are presented. Then we make a pairing of lists that contain each of our categories.\n",
    "        fin_list = []\n",
    "        for sublist in categories_together:\n",
    "            sublist = filter(lambda a: a != '', sublist)\n",
    "            search_set = set(sublist)\n",
    "            for search in search_set:\n",
    "                if len(sublist)>1:\n",
    "                    for i in range(len(sublist)):\n",
    "                        #sublist.remove('')\n",
    "                        #if sublist[i]=='':\n",
    "                         #   del sublist[i]\n",
    "                        if sublist[i] == search:\n",
    "                            fin_list.append(sublist)\n",
    "                            break\n",
    "        #make a dataframe out of all of the lists\n",
    "        orders = pd.DataFrame(fin_list)\n",
    "\n",
    "        #extract all of the categories into a list\n",
    "        categories = list(set(cat_name))\n",
    "\n",
    "        for item in categories:     #######figure out if we need this\n",
    "            item_list = []\n",
    "            item_list.append(item)\n",
    "            for columns in orders:\n",
    "                orders = orders[~orders[columns].isin(item_list)]\n",
    "                \n",
    "                \n",
    "        #replace all of the items in the dataframe that are the same as the main category so as to not duplicate\n",
    "        for item in categories:    \n",
    "            orders.replace({item:None})\n",
    "        #for every category, make a list of lists containing the orders that contain it\n",
    "        df_list = []\n",
    "        for item in categories:\n",
    "            item_specific_list = []\n",
    "            for l in fin_list:\n",
    "                if item in l:\n",
    "                    item_specific_list.append(l)\n",
    "            df_list.append(item_specific_list)\n",
    "\n",
    "        #for every order, we set the category as the index and then make a list of dataframes with each category dataframe\n",
    "        specific_dfs = []\n",
    "        for i,item in enumerate(categories):\n",
    "            lis = df_list[i]\n",
    "            for orders in lis:\n",
    "                if item in orders:\n",
    "                    dataframe = pd.DataFrame(lis)\n",
    "                    dataframe['category'] = item\n",
    "                    dataframe = dataframe.set_index('category', drop = True)\n",
    "                    col_list = list(dataframe)\n",
    "            specific_dfs.append(dataframe)\n",
    "\n",
    "        #replace all of the items in the dataframe that are the same as the main category so as to not duplicate\n",
    "        for i,item in enumerate(categories):    \n",
    "            specific_dfs[i] = specific_dfs[i].replace({item:None})\n",
    "\n",
    "#for i, df in enumerate(specific_dfs):\n",
    "        main_dict ={}\n",
    "        for i,df in enumerate(specific_dfs):\n",
    "            dictionary = {}\n",
    "            temp_cat = df.index[0]\n",
    "            dictionary[temp_cat] = df.apply(pd.value_counts).sum(axis = 1).to_dict()\n",
    "            main_dict.update(dictionary)\n",
    "        #print index_cat\n",
    "        \n",
    "        \n",
    "        #we are essentially merging the price/category dictionary with the item/quantity dictionary\n",
    "        dict3 = defaultdict(list)\n",
    "        for k, v in chain(main_dict.items(), price_id_dict.items()):\n",
    "            dict3[k].append(v)\n",
    "        \n",
    "        dict3 = dict(dict3)\n",
    "        list_of_associations = []\n",
    "        association_list = []\n",
    "\n",
    "        \n",
    "        #remove all the blank entries from every list of orders, and also remove orders that were just a single item\n",
    "        for sublist in categories_together:\n",
    "                    sublist = filter(lambda a: a != '', sublist)\n",
    "                    search_set = set(sublist)\n",
    "                    for search in search_set:\n",
    "                        if len(sublist)>1:\n",
    "                            for i in range(len(sublist)):\n",
    "                                if sublist[i] == search:\n",
    "                                    fin_list.append(sublist)\n",
    "        #make tuples of categorires and lists of orders that include those categories\n",
    "        for word in categories:\n",
    "            association_list = []\n",
    "            for lists in fin_list:\n",
    "                for i in range(len(lists)):\n",
    "                    if lists[i] == word:\n",
    "                        association_list.append(lists)\n",
    "                        break\n",
    "            word_tuple = (word, association_list)\n",
    "            list_of_associations.append(word_tuple)\n",
    "\n",
    "\n",
    "        #really tricky section, we convert all our order lists to tuples, count the tuples, then turn the Counter object into a dictionary and\n",
    "        #add to a list\n",
    "        list_with_counts = []\n",
    "        for word_tup in list_of_associations:\n",
    "            counts_list = []\n",
    "            counted = dict(Counter(tuple(e) for e in word_tup[1]))\n",
    "            for i,lists in enumerate(word_tup[1]):\n",
    "                counts_list.append(lists)\n",
    "            list_with_counts.append([word_tup[0],counted])\n",
    "            \n",
    "        dict_of_associations = dict(list_of_associations)\n",
    "        dict_of_counts = dict(list_with_counts)\n",
    "        \n",
    "        upload_dict = {}\n",
    "        \n",
    "        for keys,values in dict_of_counts.iteritems():\n",
    "            cats_list = []\n",
    "            for dicts,count in values.iteritems():\n",
    "                for cats in dicts:\n",
    "                    if keys != cats:\n",
    "                        cats_list.append(cats)\n",
    "                        cats_list = list(set(cats_list))\n",
    "            upload_dict[keys] = [cats_list,count]\n",
    "            \n",
    "        upload_dict_df = pd.DataFrame(upload_dict).transpose()\n",
    "        upload_dict_df.columns = ['order combos','combos_count']\n",
    "        upload_dict = upload_dict_df.to_dict('index')\n",
    "\n",
    "        #post to Items db\n",
    "        items.update_many({\n",
    "                          'merchant_uid': MERCHANT_ID\n",
    "                        },{\n",
    "                          '$set': {\n",
    "                              'order_quantity_info':upload_dict,\n",
    "                              'category_info':dict3\n",
    "\n",
    "                            }\n",
    "\n",
    "                          }\n",
    "                        , upsert=True)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
