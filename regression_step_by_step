1. Find variables that you want to predict
    * create any indicator variables
    * create interaction variables
    * run a few basic regressions using different techniques and rank by R2 and Adjusted R2
2. Refine
    * examine F test, MSE, Cross-Validation
3. Test for Assumptions
    * No major outliers
    * linear and additive features
    * No Auto-correlation
        * Time series data effected by this
        * Detect using ACF and PACF
        * Violates assumption that covariance between estimators is 0
        * How to fix:
            1. Least Squares with Newey-West, Robust but not BLUE
            2. Use and AR(1) model. Long run static and BLUE
    * No Multicollinearity
        * This basically is the idea that one of your x features is a linear transform of another. So if you use income and income tax paid as two separate predictors, you are likely just increasing variance with no predictive gain.
        * check that variables have reasonable effects
        * check estimator coefficients have reasonably sized effects
    * Homoskedastic
        * Plot residuals, if discernable changes in variance, heteroskedastic
        * Use Breusch Pagan test for heteroskedasticity. if you reject null, you have heteroskedasticity
        * White Test - cross product terms and interaction variables, F-test and X2
        * How to fix:
            1. White's robust standard errors -Not Best Linear Unbiased Estimators
            2. General Least Squares - BLUE, yields smaller standard error
    * Residuals Normal and centered around 0
        * Could use something like box-cox power transforms, but should just use a histogram on residuals if really concerned. With more data, this becomes less important
    * Non-stationary variables
        * Dickey-Fuller test. Reject null if stationary
        * exception. If autocorrelation also present, run dicket full on errors to determine stationarity
4. Validate
    * Run on new data
    * make sure results are intuitive
